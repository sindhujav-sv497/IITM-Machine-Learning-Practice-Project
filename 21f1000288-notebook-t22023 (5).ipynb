{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":53569,"databundleVersionId":5834979,"sourceType":"competition"}],"dockerImageVersionId":30497,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"Ti8MXLC8WmYt","execution":{"iopub.status.busy":"2023-07-30T10:04:40.568532Z","iopub.execute_input":"2023-07-30T10:04:40.569094Z","iopub.status.idle":"2023-07-30T10:04:40.575633Z","shell.execute_reply.started":"2023-07-30T10:04:40.569059Z","shell.execute_reply":"2023-07-30T10:04:40.574426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to make this notebook's output stable across runs\nnp.random.seed(42)","metadata":{"id":"SSxg3j845QyN","execution":{"iopub.status.busy":"2023-07-30T10:04:54.919414Z","iopub.execute_input":"2023-07-30T10:04:54.920001Z","iopub.status.idle":"2023-07-30T10:04:54.927791Z","shell.execute_reply.started":"2023-07-30T10:04:54.919946Z","shell.execute_reply":"2023-07-30T10:04:54.926103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show scikit-learn","metadata":{"id":"DZMEpFmGX9dP","outputId":"a7e4bccb-0090-49eb-8846-9f99b481e679"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = pd.read_csv(\"train.csv\")\n# test = pd.read_csv(\"test.csv\")\n# movie = pd.read_csv(\"movies.csv\")\n\ntrain = pd.read_csv(\"/kaggle/input/sentiment-prediction-on-movie-reviews/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/sentiment-prediction-on-movie-reviews/test.csv\")\nmovie = pd.read_csv(\"/kaggle/input/sentiment-prediction-on-movie-reviews/movies.csv\")","metadata":{"id":"T3FO8NvBWmY8","execution":{"iopub.status.busy":"2023-07-30T10:04:54.930324Z","iopub.execute_input":"2023-07-30T10:04:54.931701Z","iopub.status.idle":"2023-07-30T10:04:56.513566Z","shell.execute_reply.started":"2023-07-30T10:04:54.931640Z","shell.execute_reply":"2023-07-30T10:04:56.512154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"id":"eOiTs6fxWmY9","outputId":"6cdd72c1-e022-4b3c-9fef-562fda664cee","execution":{"iopub.status.busy":"2023-07-30T10:04:56.516075Z","iopub.execute_input":"2023-07-30T10:04:56.516467Z","iopub.status.idle":"2023-07-30T10:04:56.533513Z","shell.execute_reply.started":"2023-07-30T10:04:56.516414Z","shell.execute_reply":"2023-07-30T10:04:56.532271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movie.tail()","metadata":{"id":"WSxh7SMIWmY-","outputId":"74bc8401-f5c5-4351-8b01-534b046fe860","execution":{"iopub.status.busy":"2023-07-30T10:04:56.537083Z","iopub.execute_input":"2023-07-30T10:04:56.538427Z","iopub.status.idle":"2023-07-30T10:04:56.563218Z","shell.execute_reply.started":"2023-07-30T10:04:56.538372Z","shell.execute_reply":"2023-07-30T10:04:56.562100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"id":"8MS9ExA0WmY_","outputId":"5ed67f30-0d7f-41f3-e536-09415544bd36","execution":{"iopub.status.busy":"2023-07-30T10:04:56.564788Z","iopub.execute_input":"2023-07-30T10:04:56.566402Z","iopub.status.idle":"2023-07-30T10:04:56.586603Z","shell.execute_reply.started":"2023-07-30T10:04:56.566360Z","shell.execute_reply":"2023-07-30T10:04:56.585329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"id":"0cSnz9EgWmZA","outputId":"e5e9f4a3-86ab-4b34-bed4-a8b517e84e1b","execution":{"iopub.status.busy":"2023-07-30T10:04:56.588834Z","iopub.execute_input":"2023-07-30T10:04:56.589374Z","iopub.status.idle":"2023-07-30T10:04:56.828803Z","shell.execute_reply.started":"2023-07-30T10:04:56.589330Z","shell.execute_reply":"2023-07-30T10:04:56.827168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movie.info()","metadata":{"id":"7HrHGpQ5WmZB","outputId":"400e8967-3a12-4dbb-97f6-8dec200ac57b","execution":{"iopub.status.busy":"2023-07-30T10:04:56.831021Z","iopub.execute_input":"2023-07-30T10:04:56.831398Z","iopub.status.idle":"2023-07-30T10:04:57.350536Z","shell.execute_reply.started":"2023-07-30T10:04:56.831358Z","shell.execute_reply":"2023-07-30T10:04:57.348895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movie['rating'].unique()","metadata":{"id":"QyuNb01uWmZC","outputId":"2b679b05-a76a-428d-8100-7155bd876a5c","execution":{"iopub.status.busy":"2023-07-30T10:04:57.352616Z","iopub.execute_input":"2023-07-30T10:04:57.353135Z","iopub.status.idle":"2023-07-30T10:04:57.378213Z","shell.execute_reply.started":"2023-07-30T10:04:57.353088Z","shell.execute_reply":"2023-07-30T10:04:57.376673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"there are 10 unique values for rating , and NaN type - so we can first impute the NaN type and then apply for one hot encoding","metadata":{"id":"1emnK23tWmZE"}},{"cell_type":"code","source":"movie['genre'].unique()","metadata":{"id":"S-7pGNNrWmZI","outputId":"a02d6a31-5fea-4af4-beef-4521a9dbaff7","execution":{"iopub.status.busy":"2023-07-30T10:04:57.380256Z","iopub.execute_input":"2023-07-30T10:04:57.380726Z","iopub.status.idle":"2023-07-30T10:04:57.407563Z","shell.execute_reply.started":"2023-07-30T10:04:57.380688Z","shell.execute_reply":"2023-07-30T10:04:57.405766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 2913 unique genres for 143258 entries in the movies dataset. This means there is a proper scheme followed while filling the genre\n\nSo for genre they wrote the string as 'Action, Mystery & thriller' , and not like {'Action', 'Mystery', 'Thriller'} --> if it was like this we can easily apply multilabel binarizer.\n\nSo we need to extract the individual words from each string, convert to lower case and form a set, and then create a list from all these sets and apply multilabel binarizer (or) just let it be as one single string and apply CountVectorizer","metadata":{"id":"ofpeEh11WmZJ"}},{"cell_type":"code","source":"movie['originalLanguage'].unique()","metadata":{"id":"klWfVR7XWmZK","outputId":"6c393768-cc81-47da-f2dc-90968e942d18","execution":{"iopub.status.busy":"2023-07-30T10:04:57.410022Z","iopub.execute_input":"2023-07-30T10:04:57.410511Z","iopub.status.idle":"2023-07-30T10:04:57.438087Z","shell.execute_reply.started":"2023-07-30T10:04:57.410465Z","shell.execute_reply":"2023-07-30T10:04:57.436581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for originalLanguage there are 112 unique values and NaN - OneHotEncoding","metadata":{"id":"5so83An3WmZL"}},{"cell_type":"code","source":"len(movie['soundType'].unique())","metadata":{"id":"k-KA3pnpWmZL","outputId":"145b4c34-0e03-4e19-c350-863ad11aec0a","execution":{"iopub.status.busy":"2023-07-30T10:04:57.439893Z","iopub.execute_input":"2023-07-30T10:04:57.440302Z","iopub.status.idle":"2023-07-30T10:04:57.464018Z","shell.execute_reply.started":"2023-07-30T10:04:57.440271Z","shell.execute_reply":"2023-07-30T10:04:57.463075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for soundType there are 551 unique values out of 15917 non-null values - so similar to genre, need to fill NaN values, then extract words from each string and then apply encoding","metadata":{"id":"7eerRFChWmZM"}},{"cell_type":"code","source":"movie['ratingContents'].unique()","metadata":{"id":"_edMDd22WmZN","outputId":"66731ce1-5cc0-4bdd-c61c-c1088acf8eb4","execution":{"iopub.status.busy":"2023-07-30T10:04:57.465664Z","iopub.execute_input":"2023-07-30T10:04:57.466399Z","iopub.status.idle":"2023-07-30T10:04:57.499681Z","shell.execute_reply.started":"2023-07-30T10:04:57.466362Z","shell.execute_reply":"2023-07-30T10:04:57.498144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"in ratingContents there are 8,353 unique values out of the 13,991 non-null values,  and each value is a list of strings - after imputing, for each sample, take the list, extract the strings from list, merge to form one string, and use that string","metadata":{"id":"IR5cCfEMWmZN"}},{"cell_type":"markdown","source":"Data Type of some easily confused columns:\n\nrating type: string\n\nratingContents type: **list of strings**\n\nreleaseDateTheaters type: string\n\nreleaseDateStreaming type: string\n\nruntimeMinutes type: numpy.float64\n\nboxOffice type: string\n\ndistributor type: stringsoundType type: string","metadata":{"id":"WNaxiTtzWmZO"}},{"cell_type":"code","source":"train_col_names = train.columns\nmovie_col_names = movie.columns\nmovie_col_names","metadata":{"id":"M7GKLFFwWmZO","outputId":"7bc6a420-90b2-486f-8cdb-f75914088eae","execution":{"iopub.status.busy":"2023-07-30T10:04:57.507789Z","iopub.execute_input":"2023-07-30T10:04:57.508249Z","iopub.status.idle":"2023-07-30T10:04:57.517387Z","shell.execute_reply.started":"2023-07-30T10:04:57.508214Z","shell.execute_reply":"2023-07-30T10:04:57.516195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"train shape:\", train.shape)\nfor i in train_col_names:\n    print(i,\"null values\", train[i].isnull().sum())\n\nprint(\"\\n\")\nprint(\"movie shape:\", movie.shape)\nfor i in movie_col_names:\n    print(i,\"null values\", movie[i].isnull().sum())","metadata":{"id":"o9I6OLltWmZP","outputId":"ecc17b0e-4c47-4619-cf96-9e2a2f2341c7","execution":{"iopub.status.busy":"2023-07-30T10:04:57.519953Z","iopub.execute_input":"2023-07-30T10:04:57.520699Z","iopub.status.idle":"2023-07-30T10:04:57.780877Z","shell.execute_reply.started":"2023-07-30T10:04:57.520650Z","shell.execute_reply":"2023-07-30T10:04:57.778962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Percentage of null values: To decide which features to keep in Second Trial.\\n\")\nprint('For train dataset')\nfor i in train_col_names:\n    print(i,\"% null values\", ((train[i].isnull().sum())/162758)*100)\n\n\nprint(\"\\n\")\nprint('For movies dataset')\nprint(\"movie shape:\", movie.shape)\nfor i in movie_col_names:\n    print(i,\"% null values\", ((movie[i].isnull().sum())/143258)*100 )","metadata":{"id":"LnrCPNEB_iU2","outputId":"2fa83b75-970a-41ce-b494-a101443f3156"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For First trial use (Dropping columns on basis of Domain Knowledge)\nFrom train: isFrequentReviewer, reviewText and\nFrom movie: audienceScore, rating, ratingContents, runtimeMinutes, genre, originalLanguage, director, boxOffice, soundType**\n\nReason: From domain knowledge, we can drop movieid,reviewerName from train dataset- because moviedid is just id name, and a person's name won't have any effect on how they find a movie to be, let's say a person with name 'Sam' has posiitve sentiment for movie 'A', but doesn't mean every person named Sam will have positive sentiment for movie 'A'.\n\nFrom test dataset we can drop movieid, title, releaseDateTheaters, releaseDateStreaming, distributor - from domain knowledge - because movieid is just id and title is name of movie and won't effect how someone finds movie sentiment postive or negative, the dates of release in theaters and streaming platforms won't affect how someone finds the movie because, assumption: the data for train is collected from different websites where users can give reviews, so no matter when movie was released, one can watch movie whenever and given review. The distributor also won't affect how someone finds the movie because say, even if well reputed distributor, if the movie was bad the reviewer will give negative review only.\n\n**For Second trial use (using feature selection):**\nSome features like rating, ratingContents, boxOffice, and soundType would have to be dropped because they have more than 50% null values. However, instead of dropping them directly, I have made use of feature selection technique of SelectFromModel to select the best features.","metadata":{"id":"E9Gifq53WmZQ"}},{"cell_type":"code","source":"#audienceScore histogram\nplt.figure(figsize=(8,6))\nprint(movie['audienceScore'].mode())\nplt.title('Histogram for audienceScore')\nhi = sns.histplot(movie['audienceScore'], kde = True)\nplt.show()\n","metadata":{"id":"OaPPZaLEWmZS","outputId":"57cfdf47-039d-459c-a283-412454a833ba","execution":{"iopub.status.busy":"2023-07-30T10:04:58.114805Z","iopub.execute_input":"2023-07-30T10:04:58.115169Z","iopub.status.idle":"2023-07-30T10:04:58.727301Z","shell.execute_reply.started":"2023-07-30T10:04:58.115135Z","shell.execute_reply":"2023-07-30T10:04:58.725956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Can see the histogram distribution for audienceScore is between 0 to 100. And the values in bin 50-52 has the highest frequency of more than 2500 (with 50 being the mode).","metadata":{"id":"NxxJNBIJUDXO"}},{"cell_type":"code","source":"#runtime Minutes\nplt.figure(figsize=(8,6))\nprint(movie['runtimeMinutes'].mode())\nplt.title('Histogram for runtimeMinutes')\nsns.histplot(movie['runtimeMinutes'], kde = True)\nplt.xlim(0,400)\nplt.show()","metadata":{"id":"iuNEPodJWmZS","outputId":"77d4071a-66be-45fb-e1c5-50cc96604a4d","execution":{"iopub.status.busy":"2023-07-30T10:04:58.729128Z","iopub.execute_input":"2023-07-30T10:04:58.731245Z","iopub.status.idle":"2023-07-30T10:05:05.999562Z","shell.execute_reply.started":"2023-07-30T10:04:58.731189Z","shell.execute_reply":"2023-07-30T10:05:05.998053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The histogram for runtimeMinutes has a right tail (although the complete tail has not been shown here beacuse pf figure size constraints - the max value is 2700 minutes which is an outlier). There are more number of outliers which lie on the right side of quartile 3 ( i.e., >(Q3 + 1.5*IQR) ).","metadata":{"id":"-dWX7c_HWmZT"}},{"cell_type":"code","source":"# boxplot of audienceScore\nplt.figure(figsize=(8,6))\nplt.title('Box plot for audienceScore')\nbox = sns.boxplot(movie['audienceScore'], orient = 'h', x = movie['audienceScore'])\nmedian = movie['audienceScore'].median()\nbox.annotate(str(median), xy = (58,0))\nplt.show()","metadata":{"id":"8PFCSblGWmZT","outputId":"5f628a23-6a23-48b2-f33e-32257f4e69b3","execution":{"iopub.status.busy":"2023-07-30T10:05:06.001663Z","iopub.execute_input":"2023-07-30T10:05:06.002141Z","iopub.status.idle":"2023-07-30T10:05:06.233560Z","shell.execute_reply.started":"2023-07-30T10:05:06.002104Z","shell.execute_reply":"2023-07-30T10:05:06.232002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Can observe that audienceScore does not have outliers and the median is 57.","metadata":{"id":"NAEkxVrDQOFB"}},{"cell_type":"code","source":"# box plot of runtimeMinutes\nplt.figure(figsize=(8,6))\nplt.title('box plot for runtimeMinutes')\nbox = sns.boxplot(movie['runtimeMinutes'], orient = 'h', x=movie['runtimeMinutes'])\nmedian = movie['runtimeMinutes'].median()\n\nbox.annotate(str(median), xy = (92,0))\nprint('max:', movie['runtimeMinutes'].max())\nprint('min:', movie['runtimeMinutes'].min())\nprint('median:', median)\nplt.show()","metadata":{"id":"2h3CH0U9WmZU","outputId":"bf5e6123-4362-478d-c81e-f4a00bb6813b","execution":{"iopub.status.busy":"2023-07-30T10:05:06.235006Z","iopub.execute_input":"2023-07-30T10:05:06.235408Z","iopub.status.idle":"2023-07-30T10:05:06.487099Z","shell.execute_reply.started":"2023-07-30T10:05:06.235373Z","shell.execute_reply":"2023-07-30T10:05:06.485539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"can see some movies have very high runtime like 1000 min, >2500 min. This shows the noise in the data. The runtimeMinutes has outliers.","metadata":{"id":"UIBy1bhFWmZU"}},{"cell_type":"code","source":"# capping of outliers on runtimeMinutes\n\nruntimeMin = list(movie['runtimeMinutes'])\n# # finding the max and min values ->  upperlimit = mean+(3*standard deviation), lowerlimit = mean-(3*standard deviation)\n# upperlimit = movie2['runtimeMinutes'].mean() + 3*(movie2['runtimeMinutes'].std())\n# lowerlimit = movie2['runtimeMinutes'].mean() - 3*(movie2['runtimeMinutes'].std())\n\nq3 = movie['runtimeMinutes'].quantile(0.75)\nq1 = movie['runtimeMinutes'].quantile(0.25)\niqr = q3-q1\nmin = q1 - 1.5 * iqr\nmax = q3 + 1.5 * iqr\n\nprint(\"Quartile 1:\", q1)\nprint(\"Quartile 3:\", q3)\nprint(\"min:\", min)\nprint(\"max:\", max)\n\nprint(\"number of outliers above max:\", len(movie[movie['runtimeMinutes']>max]))\nprint(\"number of outliers below min:\", len(movie[movie['runtimeMinutes']<min]))\n\n#replacing the outliers with max and min values\nfor i in range(len(runtimeMin)):\n  if (runtimeMin[i]>max):\n    runtimeMin[i] = max\n  elif (runtimeMin[i]<min):\n    runtimeMin[i] = min\n\nmovie['runtimeMinutes'] = pd.Series(runtimeMin)\n","metadata":{"id":"Q8CM0XpPcA2G","outputId":"c0affa9f-96e0-4c33-e0bd-cdc5a18c1fb0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('max:', movie['runtimeMinutes'].max())\nprint('min: ', movie['runtimeMinutes'].min())\n\nplt.figure(figsize=(7,5))\nplt.title('Box plot of runtimeMinutes after removal of outliers')\nbox = sns.boxplot(movie['runtimeMinutes'], orient = 'h', x=movie['runtimeMinutes'])\n\nmedian = movie['runtimeMinutes'].median()\nprint('median:', median)\nbox.annotate(str(median), xy = (92,0))\nplt.show()","metadata":{"id":"wF-tyyqvkmc7","outputId":"64b2a2d4-fc05-4183-8330-0bd79932f086"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"can see that after removal of outliers, the median runtimeMinutes is 92 minutes, max: 131.5 and min 55.5 minutes.","metadata":{"id":"sb3H8Nc_tQRE"}},{"cell_type":"code","source":"#Checking the class balance\nplt.figure(figsize=(8, 6))\nplt.title('Pie chart for sentiment classes percentage')\ntrain['sentiment'].value_counts().plot(kind='pie',autopct='%0.01f%%' )","metadata":{"id":"T_yFxZ6iWmZU","outputId":"a5fe6235-29d8-4cf1-fee6-2d4d3b14d6a6","execution":{"iopub.status.busy":"2023-07-30T10:05:06.489025Z","iopub.execute_input":"2023-07-30T10:05:06.489562Z","iopub.status.idle":"2023-07-30T10:05:06.701670Z","shell.execute_reply.started":"2023-07-30T10:05:06.489515Z","shell.execute_reply":"2023-07-30T10:05:06.699511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"can see there is imbalance and positive sentiment values are almost 67% and negative sentiments are 33%.","metadata":{"id":"BvmEQ2DIWmZV"}},{"cell_type":"code","source":"dataplot = movie.iloc[:, [2,7]]\nplt.title('KDE plot for audienceScore and runtimeMinutes')\nsns.kdeplot(dataplot, bw = 0.2)","metadata":{"id":"6stRcAU6vO3v","outputId":"0e43bb4e-c6f5-43ce-e03e-13435c861f51"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"KDE plot like histogram shows the distribution of numerical features of data using PDF - here audienceScore and runtimeMinutes. After smoothing out with the bandwidth = 0.3, we can see that audienceScore gives somewhat a uniform distribution.","metadata":{"id":"FHPXtPT06ghu"}},{"cell_type":"code","source":"#removing any rows in which all values are null\ntrain2 = train.dropna(how = \"all\")\nmovie2 = movie.dropna(how = \"all\")","metadata":{"id":"KiM2TMJDWmZV","execution":{"iopub.status.busy":"2023-07-30T10:05:06.703842Z","iopub.execute_input":"2023-07-30T10:05:06.706567Z","iopub.status.idle":"2023-07-30T10:05:07.511062Z","shell.execute_reply.started":"2023-07-30T10:05:06.706490Z","shell.execute_reply":"2023-07-30T10:05:07.509546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dropping the columns 'releaseDateTheatres', 'releaseDateStreaming', and 'distributor' because\n#they do not have any impact on the movie sentiment\n\nmovie2 = movie2.drop(columns = ['releaseDateTheaters', 'releaseDateStreaming', 'distributor'] )","metadata":{"id":"tarako96BAUO","execution":{"iopub.status.busy":"2023-07-30T10:05:07.512922Z","iopub.execute_input":"2023-07-30T10:05:07.513320Z","iopub.status.idle":"2023-07-30T10:05:07.544954Z","shell.execute_reply.started":"2023-07-30T10:05:07.513285Z","shell.execute_reply":"2023-07-30T10:05:07.543500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#deleting duplicates in movie2 dataset using groupby\nmovie2 = movie2.groupby([\"movieid\"]).mean()\nmovie2.head()","metadata":{"id":"c2Do2q0RAhKh","outputId":"4082b6a6-4b55-4764-f1fa-6f62a5397048","execution":{"iopub.status.busy":"2023-07-30T10:05:07.547016Z","iopub.execute_input":"2023-07-30T10:05:07.547356Z","iopub.status.idle":"2023-07-30T10:05:07.954005Z","shell.execute_reply.started":"2023-07-30T10:05:07.547327Z","shell.execute_reply":"2023-07-30T10:05:07.952769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"since groupby on movie2 is returning only the numerical columns, taking the categorical columns into movie3 and removing the duplicates and merging it with movie2.","metadata":{"id":"fPKZiSeVagnd"}},{"cell_type":"code","source":"movie3 = movie.drop_duplicates(subset = 'movieid', keep = 'first')\nmovie3 = movie3.drop(columns = ['audienceScore', 'releaseDateTheaters', 'releaseDateStreaming', 'runtimeMinutes', 'distributor'])\nmovie3 = pd.merge(movie3, movie2, how ='left', on = ['movieid'])\nmovie3.shape","metadata":{"id":"Ve3EJt94Eeub","outputId":"c8245d38-55ba-48ad-e17b-deee316666f8","execution":{"iopub.status.busy":"2023-07-30T10:05:07.955769Z","iopub.execute_input":"2023-07-30T10:05:07.956153Z","iopub.status.idle":"2023-07-30T10:05:08.194783Z","shell.execute_reply.started":"2023-07-30T10:05:07.956120Z","shell.execute_reply":"2023-07-30T10:05:08.193325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movie3.head()","metadata":{"id":"-uwSJgZtKsBV","outputId":"4d74b2f1-758f-4df7-d8df-3dfde0099668","execution":{"iopub.status.busy":"2023-07-30T10:05:08.196801Z","iopub.execute_input":"2023-07-30T10:05:08.197229Z","iopub.status.idle":"2023-07-30T10:05:08.218589Z","shell.execute_reply.started":"2023-07-30T10:05:08.197194Z","shell.execute_reply":"2023-07-30T10:05:08.216792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movie3_columns = movie3.columns\nmovie3_columns","metadata":{"id":"6LNnsdokOOrb","outputId":"bdda2289-cba3-47ba-b067-16d2a1c82c0a","execution":{"iopub.status.busy":"2023-07-30T10:05:08.220810Z","iopub.execute_input":"2023-07-30T10:05:08.221711Z","iopub.status.idle":"2023-07-30T10:05:08.231486Z","shell.execute_reply.started":"2023-07-30T10:05:08.221631Z","shell.execute_reply":"2023-07-30T10:05:08.229705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\nct2 = ColumnTransformer([('si_movieid', 'passthrough', [0]),\n                        ('si_title', 'passthrough', [1]),\n\n                        ('si_rating', SimpleImputer(strategy = 'most_frequent'), [2]),\n                        ('si_ratingContents', SimpleImputer(strategy = 'constant', fill_value = 'na'), [3]),\n\n\n                        ('si_genre', SimpleImputer(strategy = 'most_frequent'), [4]),\n                        ('si_OL', SimpleImputer(strategy = 'most_frequent'), [5]),\n                        ('si_Director', SimpleImputer(strategy = 'most_frequent'), [6]),\n                        ('si_BO', SimpleImputer(strategy = 'constant', fill_value = 0 ), [7]),\n\n                        ('si_ST', SimpleImputer(strategy = 'constant', fill_value = 'na'), [8]),\n                        ('si_AS', SimpleImputer(strategy = 'median'), [9]),\n                        ('si_RTM', SimpleImputer(strategy = 'median'), [10])\n                       ])\n\nmovie3 = ct2.fit_transform(movie3)\nmovie3 = pd.DataFrame(movie3, columns = movie3_columns)\n#type(movie3)\nmovie3.head()","metadata":{"id":"cPgyLcNsWmZi","outputId":"e1da9bb1-e8a5-457b-aee5-ac3ade701b74","execution":{"iopub.status.busy":"2023-07-30T10:05:08.233732Z","iopub.execute_input":"2023-07-30T10:05:08.234288Z","iopub.status.idle":"2023-07-30T10:05:08.588099Z","shell.execute_reply.started":"2023-07-30T10:05:08.234242Z","shell.execute_reply":"2023-07-30T10:05:08.586342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# movie3.info()","metadata":{"id":"UapdBnOes6JJ","execution":{"iopub.status.busy":"2023-07-30T10:05:08.590808Z","iopub.execute_input":"2023-07-30T10:05:08.591356Z","iopub.status.idle":"2023-07-30T10:05:08.597207Z","shell.execute_reply.started":"2023-07-30T10:05:08.591309Z","shell.execute_reply":"2023-07-30T10:05:08.596146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting boxOffice values from string to float\n\nimport re\nbo = list(movie3['boxOffice'])\n\nfor i in range(len(bo)):\n\n  if(bo[i]!= 0):\n      bo[i] = bo[i].replace(\"$\", \"\")\n\n      if(\"K\" in bo[i]):\n        bo[i] = bo[i].replace(\"K\",\"\")\n        bo[i] = int(float(bo[i])*1000)\n\n      elif(\"M\" in bo[i]):\n        bo[i] = bo[i].replace(\"M\",\"\")\n        bo[i] = int(float(bo[i])*1000000)\n\n      elif(\"B\" in bo[i]):\n        bo[i] = bo[i].replace(\"B\",\"\")\n        bo[i] = int(float(bo[i])*1000000000)\n\n\nmovie3['boxOffice'] = pd.Series(bo)\nprint(movie3[\"boxOffice\"])","metadata":{"id":"T4fKI7_7ip_l","outputId":"9fefce47-6ab1-4b65-a78f-3ca2f76f39f6","execution":{"iopub.status.busy":"2023-07-30T10:05:08.598411Z","iopub.execute_input":"2023-07-30T10:05:08.598819Z","iopub.status.idle":"2023-07-30T10:05:08.703784Z","shell.execute_reply.started":"2023-07-30T10:05:08.598786Z","shell.execute_reply":"2023-07-30T10:05:08.702539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(movie3.iloc[12,7])","metadata":{"id":"gGuLlpSbIsHa","outputId":"ad91b177-e83b-4b9d-ef64-4c26dfaeb5fd","execution":{"iopub.status.busy":"2023-07-30T10:05:08.705730Z","iopub.execute_input":"2023-07-30T10:05:08.707229Z","iopub.status.idle":"2023-07-30T10:05:08.715558Z","shell.execute_reply.started":"2023-07-30T10:05:08.707160Z","shell.execute_reply":"2023-07-30T10:05:08.713544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#replacing the '0' values in boxOffice with median of int values\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\nct3 = ColumnTransformer([('si_movieid', 'passthrough', [0]),\n                        ('si_title', 'passthrough', [1]),\n\n                        ('si_rating', 'passthrough', [2]),\n                        ('si_ratingContents', 'passthrough', [3]),\n\n\n                        ('si_genre', 'passthrough', [4]),\n                        ('si_OL', 'passthrough', [5]),\n                        ('si_Director', 'passthrough', [6]),\n                        ('si_BO', SimpleImputer(missing_values = 0 , strategy = 'median')  , [7]),\n\n                        ('si_ST', 'passthrough', [8]),\n                        ('si_AS', 'passthrough', [9]),\n                        ('si_RTM', 'passthrough', [10])\n                       ])\n\nmovie3 = ct3.fit_transform(movie3)\nmovie3 = pd.DataFrame(movie3, columns = movie3_columns)\nprint(type(movie3))\nmovie3.head()\n\n# sim = SimpleImputer(missing_values = 0 , strategy = 'median')\n# bo_column = sim.fit_transform(np.array(movie3['boxOffice']).reshape(-1,1))\n# # print(bo_column)\n# movie3['boxOffice'] = pd.Series(list(bo_column))\n# movie3['boxOffice']","metadata":{"id":"4LLCOoNINPAo","outputId":"2ad83526-da9f-4683-ddc1-a5cec3d13cf6","execution":{"iopub.status.busy":"2023-07-30T10:05:08.717391Z","iopub.execute_input":"2023-07-30T10:05:08.717833Z","iopub.status.idle":"2023-07-30T10:05:08.917901Z","shell.execute_reply.started":"2023-07-30T10:05:08.717798Z","shell.execute_reply":"2023-07-30T10:05:08.916299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(movie3.iloc[5,:])\nprint(type(movie3.iloc[5,7]))","metadata":{"id":"DMUHyXK_UHUi","outputId":"42aabc56-322f-42f4-86f6-f9bb6e8e9586","execution":{"iopub.status.busy":"2023-07-30T10:05:08.920135Z","iopub.execute_input":"2023-07-30T10:05:08.920833Z","iopub.status.idle":"2023-07-30T10:05:08.929721Z","shell.execute_reply.started":"2023-07-30T10:05:08.920766Z","shell.execute_reply":"2023-07-30T10:05:08.928094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so the boxOffice values previously consisiting of 0 are filled with median of remaining values. All the values have been converted into float and are in currency $","metadata":{"id":"WLXiSqiWU2_F"}},{"cell_type":"code","source":"# converting ratingContents list of strings to single string\n#actually it is not list of strings, it is a string written like that- \"['Thematic Elements', 'Language', 'Injury']\"\n\nprint(len(movie3['ratingContents'].unique()))","metadata":{"id":"Kx7f9CJ9pf3a","outputId":"a81e1136-4088-495b-9754-9e3153b04e01","execution":{"iopub.status.busy":"2023-07-30T10:05:08.931640Z","iopub.execute_input":"2023-07-30T10:05:08.932150Z","iopub.status.idle":"2023-07-30T10:05:08.961503Z","shell.execute_reply.started":"2023-07-30T10:05:08.932104Z","shell.execute_reply":"2023-07-30T10:05:08.960216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"before removing duplicates the number of unique values were 8,353 unique values out of the 13,991 non-null values. Now after removing the duplicates and filling null with 'na', there are 7305 (excluding 'na') unique values.","metadata":{"id":"LNnOWOY9si6L"}},{"cell_type":"code","source":"import re\nrc = list(movie3['ratingContents'])\n\nfor i in range(len(rc)):\n  new_sentence = \" \"\n  if rc[i] == 'na':\n    rc[i] = 'na'\n    continue\n  else:\n    new_sentence = re.sub(\"|'|,\", \"\", rc[i])\n    new_sentence = new_sentence.replace('/',' ')\n    new_sentence = new_sentence.replace('[','')\n    new_sentence = new_sentence.replace(']','')\n\n  rc[i] = new_sentence\n\nmovie3['ratingContents'] = pd.Series(rc)\n#print(rc)\nprint(movie3.loc[5,:])","metadata":{"id":"jq7l8XLctRSm","outputId":"92a53274-a482-423c-c3a8-67f2cb138046","execution":{"iopub.status.busy":"2023-07-30T10:05:08.962820Z","iopub.execute_input":"2023-07-30T10:05:08.963181Z","iopub.status.idle":"2023-07-30T10:05:09.397506Z","shell.execute_reply.started":"2023-07-30T10:05:08.963150Z","shell.execute_reply":"2023-07-30T10:05:09.396161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In genre removing the \",\", \"and\", \"&\"\n\nimport re\nge = list(movie3['genre'])\n\nfor i in range(len(ge)):\n    new_sentence = \" \"\n\n    new_sentence = re.sub(\"&|'|,\", \"\", ge[i])\n    new_sentence = new_sentence.replace('and','')\n    # new_sentence = new_sentence.replace(']','')\n\n    ge[i] = new_sentence\n\nmovie3['genre'] = pd.Series(ge)\n# print(ge)\n# print(movie3.loc[5,:])","metadata":{"id":"U9t7VWo-Vh6T","execution":{"iopub.status.busy":"2023-07-30T10:05:09.399326Z","iopub.execute_input":"2023-07-30T10:05:09.399936Z","iopub.status.idle":"2023-07-30T10:05:09.825211Z","shell.execute_reply.started":"2023-07-30T10:05:09.399896Z","shell.execute_reply":"2023-07-30T10:05:09.823971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In soundType removing the \",\"\n\nimport re\nst = list(movie3['soundType'])\n\nfor i in range(len(st)):\n    new_sentence = \" \"\n\n    new_sentence = re.sub(\"&|'|,\", \"\", st[i])\n    new_sentence = new_sentence.replace('and','')\n    # new_sentence = new_sentence.replace(']','')\n\n    st[i] = new_sentence\n\nmovie3['soundType'] = pd.Series(st)\n# print(st)\n# print(movie3.loc[5,:])","metadata":{"id":"YxK-Do5Gzo5x","execution":{"iopub.status.busy":"2023-07-30T10:05:09.826766Z","iopub.execute_input":"2023-07-30T10:05:09.827160Z","iopub.status.idle":"2023-07-30T10:05:10.174657Z","shell.execute_reply.started":"2023-07-30T10:05:09.827127Z","shell.execute_reply":"2023-07-30T10:05:10.173020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movie3.head()","metadata":{"id":"UVA-_eDXYXNi","outputId":"be8c0d8a-b045-4142-9a4e-d2dc07b9a5bd","execution":{"iopub.status.busy":"2023-07-30T10:05:10.176543Z","iopub.execute_input":"2023-07-30T10:05:10.177050Z","iopub.status.idle":"2023-07-30T10:05:10.199330Z","shell.execute_reply.started":"2023-07-30T10:05:10.177006Z","shell.execute_reply":"2023-07-30T10:05:10.197702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"id":"uJ-xaNENY6mn","outputId":"f4831a28-1f8c-4abf-ec92-cd6b6c69edd3","execution":{"iopub.status.busy":"2023-07-30T10:05:10.201536Z","iopub.execute_input":"2023-07-30T10:05:10.202018Z","iopub.status.idle":"2023-07-30T10:05:10.461137Z","shell.execute_reply.started":"2023-07-30T10:05:10.201970Z","shell.execute_reply":"2023-07-30T10:05:10.459213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"id":"8EwMqUpZqI2q","outputId":"2d91b000-2574-4661-f006-335f946a9460","execution":{"iopub.status.busy":"2023-07-30T10:05:10.463041Z","iopub.execute_input":"2023-07-30T10:05:10.463426Z","iopub.status.idle":"2023-07-30T10:05:10.541143Z","shell.execute_reply.started":"2023-07-30T10:05:10.463395Z","shell.execute_reply":"2023-07-30T10:05:10.540002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"id":"xHcJ5sKWpTXH","outputId":"5fdb1739-c330-4697-b727-046a566c47e8","execution":{"iopub.status.busy":"2023-07-30T10:05:10.559095Z","iopub.execute_input":"2023-07-30T10:05:10.559581Z","iopub.status.idle":"2023-07-30T10:05:10.576685Z","shell.execute_reply.started":"2023-07-30T10:05:10.559547Z","shell.execute_reply":"2023-07-30T10:05:10.574988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count for isFrequentReviewer in train datatset\n\nnames = train['isFrequentReviewer'].value_counts().index.tolist()\nvalues = train['isFrequentReviewer'].value_counts().values.tolist()\n\n\nfig, ax = plt.subplots()\nbar_container = ax.bar(['False','True'], values)\nax.set(ylabel='counts', title='Bar plot of IsFrequentReviewer')\nax.bar_label(bar_container, fmt='{:,.0f}')\n","metadata":{"id":"Z6_pDyBeWmZR","outputId":"6cc7e224-abd2-48ce-bbf1-bb326229e54e","execution":{"iopub.status.busy":"2023-07-30T10:04:57.783117Z","iopub.execute_input":"2023-07-30T10:04:57.783686Z","iopub.status.idle":"2023-07-30T10:04:58.113037Z","shell.execute_reply.started":"2023-07-30T10:04:57.783644Z","shell.execute_reply":"2023-07-30T10:04:58.111407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that out of 162758 entries, 113189 entries the reviewer is not frequent reviewer, and 49,569 entries the reviewer is a frequent reviewer (which is 2.28:1 ratio)","metadata":{"id":"tTu0vhzEIXoS"}},{"cell_type":"code","source":"# histogram for the length of reviewText\n\nplt.figure(figsize=(7,5))\nplt.title('Histogram for reveiwText length for train data')\ntrain['reviewText'].str.len().hist()\nprint('modal length:', train['reviewText'].str.len().mode())\n# hi = sns.histplot(train['reviewText'].str.len())\nplt.show()\n\n# lengths = list(train['reviewText'])\n","metadata":{"id":"BrnXSUfSvEqO","outputId":"42f56e1d-587b-4ed7-ac10-e63fa663c5c1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We calculate length of each 'reviewText' in the 'train' DataFrame using the str.len() method, which returns the length of each string in the 'reviewText' column. Then, it calls the hist() function to plot the histogram of these lengths.\n\nThe resulting histogram will show the distribution of review text lengths. The x-axis will represent the lengths of the review texts, and the y-axis will represent the frequency (count) of review texts with each length.\n\nThis helps us understand the variability in the length of the review texts in train dataset. The most frequent length of the reviewText for train dataset is 136 words.","metadata":{"id":"WT12QgBvGEvy"}},{"cell_type":"code","source":"# Count for isTopCritic in test datatset\n\nnames = test['isTopCritic'].value_counts().index.tolist()\nvalues = test['isTopCritic'].value_counts().values.tolist()\n\n\nfig, ax = plt.subplots()\nbar_container = ax.bar(['False','True'], values)\nax.set(ylabel='counts', title='Bar plot of IsTopCritic')\nax.bar_label(bar_container, fmt='{:,.0f}')","metadata":{"id":"eueVcWt5EY9O","outputId":"1c57cd50-20d2-4acb-ff44-2a721ab2f731"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that out of 55315 entries, 38,428 entries the reviewer is not top critic and 16,887 times the reviewer is top critic (which is 2.27:1 ratio --> almost same as that of isFrequentReviewer for train)","metadata":{"id":"b2CwYwUDIw04"}},{"cell_type":"code","source":"# histogram for the length of reviewText in test\n\nplt.figure(figsize=(7,5))\nplt.title('Histogram for reveiwText length for test data')\ntest['reviewText'].str.len().hist()\nprint('modal length:', test['reviewText'].str.len().mode())\n# hi = sns.histplot(test['reviewText'].str.len())\nplt.show()\n","metadata":{"id":"0brhfPEZE3xK","outputId":"5f445e3e-6874-48b3-ce87-77ac326191b7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_col_names = train.columns\n# print(train_col_names)\nfor i in train_col_names:\n    print(i,\"null values\", train[i].isnull().sum())","metadata":{"id":"-Zc7_Ll2ZXe9","outputId":"f1d11ab5-ebd3-4806-d514-15420d5a6958","execution":{"iopub.status.busy":"2023-07-30T10:05:10.579693Z","iopub.execute_input":"2023-07-30T10:05:10.581679Z","iopub.status.idle":"2023-07-30T10:05:10.681189Z","shell.execute_reply.started":"2023-07-30T10:05:10.581551Z","shell.execute_reply":"2023-07-30T10:05:10.679475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imputing null values in train dataset - as only reviewText contains null values, using fillna\ntrain['reviewText']= train['reviewText'].fillna('na')\ntest['reviewText'] = test['reviewText'].fillna('na')\nprint(train.iloc[113,:])","metadata":{"id":"goPOInjKYgbw","outputId":"eea4aebf-0518-4a94-b602-f331392e622c","execution":{"iopub.status.busy":"2023-07-30T10:05:10.682967Z","iopub.execute_input":"2023-07-30T10:05:10.683391Z","iopub.status.idle":"2023-07-30T10:05:10.753466Z","shell.execute_reply.started":"2023-07-30T10:05:10.683359Z","shell.execute_reply":"2023-07-30T10:05:10.751737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_unique_movieid = train['movieid'].unique()\nlen(train_unique_movieid)","metadata":{"id":"byVrfWVieb8x","outputId":"f1afbc62-4ed6-47a1-9c76-fd02b2579f0e","execution":{"iopub.status.busy":"2023-07-30T10:05:10.754769Z","iopub.execute_input":"2023-07-30T10:05:10.755140Z","iopub.status.idle":"2023-07-30T10:05:10.803654Z","shell.execute_reply.started":"2023-07-30T10:05:10.755102Z","shell.execute_reply":"2023-07-30T10:05:10.802045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_unique_reviewer = train['reviewerName'].unique()\nlen(train_unique_reviewer)","metadata":{"id":"PkEnpjxbffkK","outputId":"055d5657-fa8c-4109-c9a3-1f41f9f1db3e","execution":{"iopub.status.busy":"2023-07-30T10:05:10.805432Z","iopub.execute_input":"2023-07-30T10:05:10.805953Z","iopub.status.idle":"2023-07-30T10:05:10.834129Z","shell.execute_reply.started":"2023-07-30T10:05:10.805919Z","shell.execute_reply":"2023-07-30T10:05:10.832047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2 = train.copy()\ndupes_on_same = train2.drop_duplicates(subset = ['movieid', 'reviewerName'])\ndupes_on_same2 = train2.drop_duplicates(subset = ['movieid', 'reviewerName', 'isFrequentReviewer', 'reviewText', 'sentiment'])\nprint(len(dupes_on_same))\nprint(len(dupes_on_same2))","metadata":{"id":"ltEOlmuigfxF","outputId":"94492f90-b1db-4292-9a28-f7a6b728ed9c","execution":{"iopub.status.busy":"2023-07-30T10:05:10.835853Z","iopub.execute_input":"2023-07-30T10:05:10.836227Z","iopub.status.idle":"2023-07-30T10:05:11.177508Z","shell.execute_reply.started":"2023-07-30T10:05:10.836195Z","shell.execute_reply":"2023-07-30T10:05:11.175315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1) in the train dataset, there are only 16,812 unique movie ids out of 1,62,758 entries - can infer that for same movie id several people gave reviews, or/and duplicates of the row exist (from point 3a), and/or same movieid and same reviewer but different entries for isFrequentReviwer, reviewText or sentiment (fom point 3b)\n\n2) also can see that there are only 4482 unique reviewers who gave reviews - could be because that same reviewer reviewed several different movies, or/and duplicates of the row exist\n\n3) on dropping rows with same values in columns 'movieid' and 'reviewerName', it results in dataframe with 161205 rows, AND,\n\non dropping the rows on which all columns were considered for checking for duplicates, it resulted in 161640 rows - meaning :\n a) 1118 rows were duplicate.  \n b) there are rows with movieid and reviwerName same, but different entries for 'isFrequentReviewer', 'reviewText' or 'sentiment' - that is why is is resulting in different number of rows when duplicates are removed considering all features and when duplicates are removed considering only the movieid and reviewerName","metadata":{"id":"6kayWehGfKRA"}},{"cell_type":"code","source":"#merging train and movie3 datasets\nmerged_train = pd.merge(movie3, train, how ='right', on = ['movieid'])\nmerged_test = pd.merge(movie3, test, how='right', on = ['movieid'])\nprint(\"shape of merged train dataframe:\", merged_train.shape)\nprint(\"shape of merged test dataframe:\", merged_test.shape)","metadata":{"id":"oufGfF78dhnU","outputId":"b3d4ffb6-3883-4fb2-8b4f-a3b5ead3e44b","execution":{"iopub.status.busy":"2023-07-30T10:05:11.179285Z","iopub.execute_input":"2023-07-30T10:05:11.180793Z","iopub.status.idle":"2023-07-30T10:05:11.744587Z","shell.execute_reply.started":"2023-07-30T10:05:11.180733Z","shell.execute_reply":"2023-07-30T10:05:11.742825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train.head()","metadata":{"id":"ZHatA0eUeRUs","outputId":"64f8c097-46f9-4107-a857-ec2855645606","execution":{"iopub.status.busy":"2023-07-30T10:05:11.746497Z","iopub.execute_input":"2023-07-30T10:05:11.746999Z","iopub.status.idle":"2023-07-30T10:05:11.775225Z","shell.execute_reply.started":"2023-07-30T10:05:11.746953Z","shell.execute_reply":"2023-07-30T10:05:11.772222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dropping columns movieid, title and revierName as they do not have any effect on the sentiment\nmerged_train = merged_train.drop(columns = ['movieid', 'title', 'reviewerName'])\nmerged_test = merged_test.drop(columns = ['movieid', 'title', 'reviewerName'])\nprint(\"merged_train shape:\", merged_train.shape)\nprint(\"merged_test_shape:\", merged_test.shape)","metadata":{"id":"DI7a5ZAJsUDV","outputId":"b87e73dc-57bf-481b-cc83-c3b7cda98b23","execution":{"iopub.status.busy":"2023-07-30T10:05:11.778360Z","iopub.execute_input":"2023-07-30T10:05:11.778843Z","iopub.status.idle":"2023-07-30T10:05:11.836111Z","shell.execute_reply.started":"2023-07-30T10:05:11.778808Z","shell.execute_reply":"2023-07-30T10:05:11.834535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train.head()","metadata":{"id":"A_SXGo5TuBZG","outputId":"6c2706de-5882-40d1-bab4-726b580282a9","execution":{"iopub.status.busy":"2023-07-30T10:05:11.839904Z","iopub.execute_input":"2023-07-30T10:05:11.840320Z","iopub.status.idle":"2023-07-30T10:05:11.863103Z","shell.execute_reply.started":"2023-07-30T10:05:11.840288Z","shell.execute_reply":"2023-07-30T10:05:11.861524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Text preprocessing before giving to countVectorizer\nimport re\n\ndef remove_newlines_tabs(text):\n    \"\"\"\n    This function will remove all the occurrences of newlines, tabs, and combinations like: \\\\n, \\\\.\n\n    \"\"\"\n\n    # Replacing all the occurrences of \\n,\\\\n,\\t,\\\\ with a space.\n    Formatted_text = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n    return Formatted_text\n\n\n\n# Code for removing repeated characters and punctuations\n\ndef reducing_incorrect_character_repeatation(text):\n    \"\"\"\n    This Function will reduce repeatition to two characters\n    for alphabets and to one character for punctuations.\n\n    arguments:\n         input_text: \"text\" of type \"String\".\n\n    return:\n        value: Finally formatted text with alphabets repeating to\n        two characters & punctuations limited to one repeatition\n\n    Example:\n    Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n    Output : Reallyy, Greeaatt !?.;:)\n\n    \"\"\"\n    # Pattern matching for all case alphabets\n    Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n\n    # Limiting all the  repeatation to two characters.\n    Formatted_text = Pattern_alpha.sub(r\"\\1\\1\", text)\n\n    # Pattern matching for all the punctuations that can occur\n    Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n\n    # Limiting punctuations in previously formatted string to only one.\n    Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_text)\n\n    # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.\n    Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n    return Final_Formatted\n\nCONTRACTION_MAP = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\",\n}\n\n# The code for expanding contraction words\ndef expand_contractions(text, contraction_mapping =  CONTRACTION_MAP):\n    \"\"\"expand shortened words to the actual form.\n       e.g. don't to do not\n\n       arguments:\n            input_text: \"text\" of type \"String\".\n\n       return:\n            value: Text with expanded form of shorthened words.\n\n       Example:\n       Input : ain't, aren't, can't, cause, can't've\n       Output :  is not, are not, cannot, because, cannot have\n\n     \"\"\"\n    # Tokenizing text into tokens.\n    list_Of_tokens = text.split(' ')\n\n    # Check whether Word is in lidt_Of_tokens or not.\n    for Word in list_Of_tokens:\n        # Check whether found word is in dictionary \"Contraction Map\" or not as a key.\n         if Word in CONTRACTION_MAP:\n                # If Word is present in both dictionary & list_Of_tokens, replace that word with the key value.\n                list_Of_tokens = [item.replace(Word, CONTRACTION_MAP[Word]) for item in list_Of_tokens]\n\n    # Converting list of tokens to String.\n    String_Of_tokens = ' '.join(str(e) for e in list_Of_tokens)\n    return String_Of_tokens\n\n\n\n\n## Function to preprocess text by regex and some special symbols\n\ndef preprocess_text(text_messages):\n\n\n  processed = text_messages.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n                                  'emailaddress')\n\n  # Replace URLs with 'webaddress'\n  processed = processed.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n                                    'webaddress')\n\n  # Replace money symbols with 'moneysymb' ( can by typed with ALT key + 156)\n  processed = processed.replace(r'|\\$', 'moneysymb')\n\n  # Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\n  processed = processed.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n                                    'phonenumbr')\n\n  # Replace numbers with 'numbr'\n  processed = processed.replace(r'\\d+(\\.\\d+)?', 'numbr')\n\n  processed = processed.replace(r'[^\\w\\d\\s]', ' ')\n\n# Replace whitespace between terms with a single space\n  processed = processed.replace(r'\\s+', ' ')\n\n# Remove leading and trailing whitespace\n  processed = processed.replace(r'^\\s+|\\s+?$', '')\n\n# change words to lower case - Hello, HELLO, hello are all the same word\n  processed = processed.lower()\n\n\n  return processed\n\n\n\n# Writing main function to merge all the preprocessing steps.\ndef text_preprocessing(text, accented_chars=True, contractions=True, newlines_tabs=True, repeatition=True,\n                       mis_spell=True, remove_html=True, preprocess = True, lemma = True):\n    \"\"\"\n    This function will preprocess input text and return\n    the clean text.\n    \"\"\"\n\n    if newlines_tabs == True: #remove newlines & tabs.\n        Data = remove_newlines_tabs(text)\n\n    # if remove_html == True: #remove html tags\n    #     Data = strip_html_tags(Data)\n\n    # if accented_chars == True: #remove accented characters\n    #     Data = accented_characters_removal(Data)\n\n    if repeatition == True: #Reduce repeatitions\n        Data = reducing_incorrect_character_repeatation(Data)\n\n    if contractions == True: #expand contractions\n        Data = expand_contractions(Data)\n\n    if preprocess == True:\n        Data = preprocess_text(Data)\n\n\n    return Data","metadata":{"id":"xEv1UHSIuxJe","execution":{"iopub.status.busy":"2023-07-30T10:05:11.865530Z","iopub.execute_input":"2023-07-30T10:05:11.865924Z","iopub.status.idle":"2023-07-30T10:05:11.901539Z","shell.execute_reply.started":"2023-07-30T10:05:11.865895Z","shell.execute_reply":"2023-07-30T10:05:11.899414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#applying text preprocessing on ratingContents, genre, soundType, reviewText\n\nmerged_train['ratingContents'] = merged_train['ratingContents'].apply(text_preprocessing)\nmerged_train['genre'] = merged_train['genre'].apply(text_preprocessing)\nmerged_train['soundType'] = merged_train['soundType'].apply(text_preprocessing)\nmerged_train['reviewText'] = merged_train['reviewText'].apply(text_preprocessing)\n\nmerged_test['ratingContents'] = merged_test['ratingContents'].apply(text_preprocessing)\nmerged_test['genre'] = merged_test['genre'].apply(text_preprocessing)\nmerged_test['soundType'] = merged_test['soundType'].apply(text_preprocessing)\nmerged_test['reviewText'] = merged_test['reviewText'].apply(text_preprocessing)\n","metadata":{"id":"hUTlzulTwUIO","execution":{"iopub.status.busy":"2023-07-30T10:05:11.903674Z","iopub.execute_input":"2023-07-30T10:05:11.904162Z","iopub.status.idle":"2023-07-30T10:05:37.627179Z","shell.execute_reply.started":"2023-07-30T10:05:11.904118Z","shell.execute_reply":"2023-07-30T10:05:37.625815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#separating the sentiment column into labels\n\nmerged_train_labels = np.array(merged_train['sentiment'])\nmerged_train = merged_train.drop(columns = ['sentiment'])\n\nprint(\"merged train labels:\", merged_train_labels)\nprint(\"merged_train shape:\", merged_train.shape)\n\nprint(\"merged_test shape:\", merged_test.shape)","metadata":{"id":"oikWUjM64s0r","outputId":"ea0761b2-966b-44b4-e0d3-3e8b2e3f5f99","execution":{"iopub.status.busy":"2023-07-30T10:05:37.629276Z","iopub.execute_input":"2023-07-30T10:05:37.629981Z","iopub.status.idle":"2023-07-30T10:05:37.680706Z","shell.execute_reply.started":"2023-07-30T10:05:37.629942Z","shell.execute_reply":"2023-07-30T10:05:37.679294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train.head()","metadata":{"id":"lSavRo8ixEvk","outputId":"d6ce823f-3817-40fa-9f84-c34a5a8eb1e1","execution":{"iopub.status.busy":"2023-07-30T10:05:37.682310Z","iopub.execute_input":"2023-07-30T10:05:37.682668Z","iopub.status.idle":"2023-07-30T10:05:37.705265Z","shell.execute_reply.started":"2023-07-30T10:05:37.682638Z","shell.execute_reply":"2023-07-30T10:05:37.703341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#renaming the isTopCritic to isFrequentReviewr\nmerged_test = merged_test.rename(columns = {'isTopCritic': 'isFrequentReviewer'})","metadata":{"id":"MFmoNk3-977o","execution":{"iopub.status.busy":"2023-07-30T10:05:37.707074Z","iopub.execute_input":"2023-07-30T10:05:37.707525Z","iopub.status.idle":"2023-07-30T10:05:37.771100Z","shell.execute_reply.started":"2023-07-30T10:05:37.707483Z","shell.execute_reply":"2023-07-30T10:05:37.769005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split the data into train and test and then apply the transformations -> fit_transform on train and only transform on the validation\n# make these transformations in pipeline\n# use the pipeline for train, validation and test set\n\n# transformations:\n# OHE on rating, originalLanguage, director, isFrequentReviewer\n# CountVectorizer on ratingContents, genre, reviewText, soundType\n# MinMaxScaler on boxOffice, audienceScore and runtimeMinutes\n# LabelBinarizer on y - sentiment column\n","metadata":{"id":"6XF3Pf4lyP-A","execution":{"iopub.status.busy":"2023-07-30T10:05:37.772865Z","iopub.execute_input":"2023-07-30T10:05:37.774051Z","iopub.status.idle":"2023-07-30T10:05:37.779013Z","shell.execute_reply.started":"2023-07-30T10:05:37.774012Z","shell.execute_reply":"2023-07-30T10:05:37.777993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, LabelBinarizer, MinMaxScaler\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nmerged_X_train, merged_X_val, merged_y_train, merged_y_val = train_test_split(merged_train, merged_train_labels, test_size = 0.3, random_state = 42)\n\n#there are 3 sets:\n# training : merged_X_train, merged_y_train\n# validation: merged_X_val, merged_y_val\n# test: merged_test","metadata":{"id":"W7f6fXRB3F2T","execution":{"iopub.status.busy":"2023-07-30T10:05:37.780359Z","iopub.execute_input":"2023-07-30T10:05:37.781669Z","iopub.status.idle":"2023-07-30T10:05:38.079358Z","shell.execute_reply.started":"2023-07-30T10:05:37.781617Z","shell.execute_reply":"2023-07-30T10:05:38.077889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"merged_X_train shape:\", merged_X_train.shape)\nprint(\"merged_X_val shape:\", merged_X_val.shape)","metadata":{"id":"g35_kroUEDce","outputId":"b02984b2-662b-4b3c-c472-3e6ef1676360","execution":{"iopub.status.busy":"2023-07-30T10:05:38.081251Z","iopub.execute_input":"2023-07-30T10:05:38.081685Z","iopub.status.idle":"2023-07-30T10:05:38.088949Z","shell.execute_reply.started":"2023-07-30T10:05:38.081652Z","shell.execute_reply":"2023-07-30T10:05:38.087691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CountVectorizer\n\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# # vectorizer = TfidfVectorizer(analyzer= 'word', tokenizer = None, max_df = 1, ngram_range = (1,2))\n# tried using TfidfVectorizer but it was giving lower f1-score than that using CountVectorizer\n\nvectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None)\n#kept stop_words = 'english' initially. On removing this parameter, the score of SVM increased from 0.80 to 0.81\n#this could be beacuse some stop words are important and removal of them might change the sentiment sometimes","metadata":{"id":"lXJGAVtX9-6W","execution":{"iopub.status.busy":"2023-07-30T10:05:38.090675Z","iopub.execute_input":"2023-07-30T10:05:38.091121Z","iopub.status.idle":"2023-07-30T10:05:38.106313Z","shell.execute_reply.started":"2023-07-30T10:05:38.091086Z","shell.execute_reply":"2023-07-30T10:05:38.104548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(merged_X_train)","metadata":{"id":"PtikFwIeH-nC","outputId":"43a72e7f-c9d8-448d-d89b-68828fca6c84","execution":{"iopub.status.busy":"2023-07-30T10:05:38.108458Z","iopub.execute_input":"2023-07-30T10:05:38.108868Z","iopub.status.idle":"2023-07-30T10:05:38.120454Z","shell.execute_reply.started":"2023-07-30T10:05:38.108837Z","shell.execute_reply":"2023-07-30T10:05:38.119452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_X_train['ratingContents']","metadata":{"id":"cIDg28Hl18cL","outputId":"b662e0bc-b6e9-424d-a5a0-7e23dffd9c5b","execution":{"iopub.status.busy":"2023-07-30T10:05:38.121780Z","iopub.execute_input":"2023-07-30T10:05:38.123270Z","iopub.status.idle":"2023-07-30T10:05:38.140279Z","shell.execute_reply.started":"2023-07-30T10:05:38.123199Z","shell.execute_reply":"2023-07-30T10:05:38.138777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_test","metadata":{"id":"WMNvlUFO_Lzo","outputId":"31c30661-de0b-4d84-8016-4cc9adfe9ef9","execution":{"iopub.status.busy":"2023-07-30T10:05:38.142624Z","iopub.execute_input":"2023-07-30T10:05:38.143083Z","iopub.status.idle":"2023-07-30T10:05:38.169159Z","shell.execute_reply.started":"2023-07-30T10:05:38.143041Z","shell.execute_reply":"2023-07-30T10:05:38.167733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ohe_columns = ['rating', 'originalLanguage', 'director', 'isFrequentReviewer']\n# cv_columns = ['ratingContents', 'genre', 'reviewText', 'soundType']\nsc_columns = ['boxOffice', 'audienceScore', 'runtimeMinutes']\n#oe_columns = ['isFrequentReviewer']\n\nratCon_t = vectorizer.fit_transform(merged_X_train['ratingContents'])\nratCon_v = vectorizer.transform(merged_X_val['ratingContents'])\nratCon_T = vectorizer.transform(merged_test['ratingContents'])\n\ngen_t = vectorizer.fit_transform(merged_X_train['genre'])\ngen_v = vectorizer.transform(merged_X_val['genre'])\ngen_T = vectorizer.transform(merged_test['genre'])\n\nrevtext_t= vectorizer.fit_transform(merged_X_train['reviewText'])\nrevtext_v= vectorizer.transform(merged_X_val['reviewText'])\nrevtext_T= vectorizer.transform(merged_test['reviewText'])\n\nst_t = vectorizer.fit_transform(merged_X_train['soundType'])\nst_v = vectorizer.transform(merged_X_val['soundType'])\nst_T = vectorizer.transform(merged_test['soundType'])\n\nmerged_X_train_dropped = merged_X_train.drop(columns = ['ratingContents', 'genre', 'reviewText', 'soundType'])\nmerged_X_val_dropped = merged_X_val.drop(columns = ['ratingContents', 'genre', 'reviewText', 'soundType'])\nmerged_test_dropped = merged_test.drop(columns = ['ratingContents', 'genre', 'reviewText', 'soundType'])\n\nct = ColumnTransformer([('ohe', OneHotEncoder(handle_unknown = 'ignore'), ohe_columns),\n                        ('minmaxscaler', MinMaxScaler(), sc_columns),\n                        ],  remainder = 'passthrough')\n","metadata":{"id":"VojooQIE9P2G","execution":{"iopub.status.busy":"2023-07-30T10:05:38.171691Z","iopub.execute_input":"2023-07-30T10:05:38.172140Z","iopub.status.idle":"2023-07-30T10:05:55.515252Z","shell.execute_reply.started":"2023-07-30T10:05:38.172105Z","shell.execute_reply":"2023-07-30T10:05:55.511728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#converting merged_X_train into sparse\n\nmerged_X_train_transformed = ct.fit_transform(merged_X_train_dropped)\nmerged_X_val_transformed = ct.transform(merged_X_val_dropped)\nmerged_test_transformed = ct.transform(merged_test_dropped)","metadata":{"id":"GYMIk5Nw3H0V","execution":{"iopub.status.busy":"2023-07-30T10:05:55.518531Z","iopub.execute_input":"2023-07-30T10:05:55.518964Z","iopub.status.idle":"2023-07-30T10:05:56.872106Z","shell.execute_reply.started":"2023-07-30T10:05:55.518931Z","shell.execute_reply":"2023-07-30T10:05:56.869820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_X_train_transformed.shape","metadata":{"id":"CxRsbKweF1dj","outputId":"4e54ec00-27dd-43b1-df05-3a3db3c3a5de","execution":{"iopub.status.busy":"2023-07-30T10:05:56.874535Z","iopub.execute_input":"2023-07-30T10:05:56.875109Z","iopub.status.idle":"2023-07-30T10:05:56.889621Z","shell.execute_reply.started":"2023-07-30T10:05:56.875059Z","shell.execute_reply":"2023-07-30T10:05:56.887779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_X_train_transformed","metadata":{"id":"L4HJJ3Ib6u96","outputId":"9e633dd1-32f4-4254-db4a-84827eccd485","execution":{"iopub.status.busy":"2023-07-30T10:05:56.891649Z","iopub.execute_input":"2023-07-30T10:05:56.892466Z","iopub.status.idle":"2023-07-30T10:05:56.905734Z","shell.execute_reply.started":"2023-07-30T10:05:56.892408Z","shell.execute_reply":"2023-07-30T10:05:56.904028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.sparse import csr_matrix\nmerged_X_train_sparse = csr_matrix(merged_X_train_transformed)\nmerged_X_val_sparse = csr_matrix(merged_X_val_transformed)\nmerged_test_sparse = csr_matrix(merged_test_transformed)\n","metadata":{"id":"0huzfn3I6LQw","execution":{"iopub.status.busy":"2023-07-30T10:05:56.909552Z","iopub.execute_input":"2023-07-30T10:05:56.910126Z","iopub.status.idle":"2023-07-30T10:05:56.927208Z","shell.execute_reply.started":"2023-07-30T10:05:56.910074Z","shell.execute_reply":"2023-07-30T10:05:56.923698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_X_train_sparse.shape","metadata":{"id":"pWX561A2Dlnu","outputId":"ab05f600-1cbd-4da0-ac9b-2f25adea33ab","execution":{"iopub.status.busy":"2023-07-30T10:05:56.930090Z","iopub.execute_input":"2023-07-30T10:05:56.932045Z","iopub.status.idle":"2023-07-30T10:05:56.949091Z","shell.execute_reply.started":"2023-07-30T10:05:56.931982Z","shell.execute_reply":"2023-07-30T10:05:56.947800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ratCon_v.shape)\nprint(gen_v.shape)\nprint(st_v.shape)\nprint(revtext_v.shape)","metadata":{"id":"Z8DbOdR1GXQK","outputId":"f4be9475-b663-46c0-ab53-f581a667a661","execution":{"iopub.status.busy":"2023-07-30T10:05:56.951074Z","iopub.execute_input":"2023-07-30T10:05:56.951790Z","iopub.status.idle":"2023-07-30T10:05:56.971507Z","shell.execute_reply.started":"2023-07-30T10:05:56.951749Z","shell.execute_reply":"2023-07-30T10:05:56.969670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#concatenating the count vectorised columns and other columns\nfrom scipy.sparse import csr_matrix, hstack\nmerged_X_train_final = hstack([ratCon_t, gen_t, st_t, merged_X_train_sparse, revtext_t]) #sparse and concatenated\nmerged_X_val_final = hstack([ratCon_v, gen_v, st_v, merged_X_val_sparse, revtext_v])  #sparse and concatenated\nmerged_test_final = hstack([ratCon_T, gen_T, st_T, merged_test_sparse, revtext_T])   #sparse and concatenated","metadata":{"id":"fY6FKWaL7UxH","execution":{"iopub.status.busy":"2023-07-30T10:05:56.973488Z","iopub.execute_input":"2023-07-30T10:05:56.975690Z","iopub.status.idle":"2023-07-30T10:05:57.148979Z","shell.execute_reply.started":"2023-07-30T10:05:56.975624Z","shell.execute_reply":"2023-07-30T10:05:57.145668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_X_train_final.shape","metadata":{"id":"Faq09hp68ffb","outputId":"7baec9b8-b1c9-46fb-a9eb-7afbda67c38f","execution":{"iopub.status.busy":"2023-07-30T10:05:57.153319Z","iopub.execute_input":"2023-07-30T10:05:57.153826Z","iopub.status.idle":"2023-07-30T10:05:57.172208Z","shell.execute_reply.started":"2023-07-30T10:05:57.153791Z","shell.execute_reply":"2023-07-30T10:05:57.168809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_test_final.shape","metadata":{"id":"p3j8vx949Fm2","outputId":"3a2557c3-4817-4481-c318-cadb3588daad","execution":{"iopub.status.busy":"2023-07-30T10:05:57.175787Z","iopub.execute_input":"2023-07-30T10:05:57.177391Z","iopub.status.idle":"2023-07-30T10:05:57.192615Z","shell.execute_reply.started":"2023-07-30T10:05:57.177350Z","shell.execute_reply":"2023-07-30T10:05:57.189333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y preprocessing\nfrom sklearn.preprocessing import LabelBinarizer\nlb = LabelBinarizer(sparse_output = True)\n\nmerged_y_train_final = lb.fit_transform(merged_y_train)\nmerged_y_val_final = lb.transform(merged_y_val)","metadata":{"id":"HFGDHgnrJrvm","execution":{"iopub.status.busy":"2023-07-30T10:05:57.194710Z","iopub.execute_input":"2023-07-30T10:05:57.195608Z","iopub.status.idle":"2023-07-30T10:05:57.211369Z","shell.execute_reply.started":"2023-07-30T10:05:57.195433Z","shell.execute_reply":"2023-07-30T10:05:57.208057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merged_X_train_final, merged_y_train_final\n# merged_X_val_final, merged_y_val_final\n# merged_test_final","metadata":{"id":"Tca_FPQnOwpj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_y_train_final","metadata":{"id":"KDQ_iM9RKK01","outputId":"d92b34ff-8339-4f56-831e-5b28a27f5522","execution":{"iopub.status.busy":"2023-07-30T10:05:57.215175Z","iopub.execute_input":"2023-07-30T10:05:57.216020Z","iopub.status.idle":"2023-07-30T10:05:57.234061Z","shell.execute_reply.started":"2023-07-30T10:05:57.215938Z","shell.execute_reply":"2023-07-30T10:05:57.232505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SVC:**","metadata":{"id":"eYtt_oSzxxZk"}},{"cell_type":"code","source":"#Linear SVC\n\nfrom sklearn.svm import LinearSVC\n\nsvc = LinearSVC(C = 0.01, random_state =46, max_iter = 946)\nsvc.fit(merged_X_train_final, merged_y_train_final.toarray())\n","metadata":{"id":"TUbujwvmJJVu","execution":{"iopub.status.busy":"2023-07-30T10:05:57.236276Z","iopub.execute_input":"2023-07-30T10:05:57.237129Z","iopub.status.idle":"2023-07-30T10:05:57.256479Z","shell.execute_reply.started":"2023-07-30T10:05:57.237065Z","shell.execute_reply":"2023-07-30T10:05:57.253131Z"},"outputId":"f8fddf10-695c-4f49-805c-551c824a3472","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_svc = svc.predict(merged_X_val_final)","metadata":{"id":"44i5EM2kJeMJ","execution":{"iopub.status.busy":"2023-07-30T10:05:57.257901Z","iopub.execute_input":"2023-07-30T10:05:57.258286Z","iopub.status.idle":"2023-07-30T10:05:57.272561Z","shell.execute_reply.started":"2023-07-30T10:05:57.258255Z","shell.execute_reply":"2023-07-30T10:05:57.270031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, f1_score\ncp = classification_report(merged_y_train_final.toarray(), svc.predict(merged_X_train_final))\ncm = confusion_matrix(merged_y_train_final.toarray(), svc.predict(merged_X_train_final))\nlinsvctr = f1_score(merged_y_train_final.toarray(), svc.predict(merged_X_train_final), average = 'weighted')\nprint(cp)\nprint('The confusion matrix:')\nprint(cm)","metadata":{"id":"mFORMEvSeDUh","outputId":"56ce3c72-b616-4450-920c-66635d6b2cea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, f1_score\ncp = classification_report(merged_y_val_final.toarray(), y_pred_svc)\ncm = confusion_matrix(merged_y_val_final.toarray(), y_pred_svc)\nlinsvc = f1_score(merged_y_val_final.toarray(), y_pred_svc, average = 'weighted')\n\nprint(cp)\nprint('The confusion matrix:')\nprint(cm)","metadata":{"id":"qyNpucquJetp","execution":{"iopub.status.busy":"2023-07-30T10:05:57.276798Z","iopub.execute_input":"2023-07-30T10:05:57.277270Z","iopub.status.idle":"2023-07-30T10:05:57.291380Z","shell.execute_reply.started":"2023-07-30T10:05:57.277237Z","shell.execute_reply":"2023-07-30T10:05:57.289393Z"},"outputId":"d2d92be3-fd01-46e3-dd32-aa80684ab8c6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For basic LinearSVC (without HPT), f1-score for train is 0.95 and test is 0.78 -> so model is overfitting.\n\nNext tried increasing regularisation (C from 1 to 0.01) and decreasing max_iter (1000 to 800) -> reduced overfitting (train:0.85, test:0.8)\n\nAfter performing HPT, found the best values max_iter =894 and C=0.01 -> got the same f1-score of train: 0.85 and test:0.80 with that.\n\nAfter removing stop_words parameter and doing HPT --> max_iter =946 and C=0.01 --> got f1-score train:0.85 and test: 0.81 with that.","metadata":{"id":"KUSfuuY_1x1t"}},{"cell_type":"code","source":"# #Linear SVC with HPT\n# from sklearn.svm import LinearSVC\n\n# svc2 = LinearSVC(random_state = 46)\n# from sklearn.model_selection import RandomizedSearchCV\n# param_dist = {'C': [0.001, 0.01, 0.1, 1],\n#               'max_iter': np.arange(800, 1000)}     #for LinearSVC default C=1, max_iter=1000, and loss=squared_hinge\n\n# rs_svc = RandomizedSearchCV(svc2, param_distributions = param_dist, scoring = 'f1_micro', cv = 5, n_jobs =-1)\n# rs_svc.fit(merged_X_train_final, merged_y_train_final.toarray())","metadata":{"id":"TiggzT0Femid"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"best score:\", rs_svc.best_score_)\n# print(\"best parameters:\", rs_svc.best_params_)","metadata":{"id":"4lK9oD9Dgr_z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best values for parameters C and max_iter as found by HPT in order to reduce overfitting are 894 and 0.01 respectively (with best score: 0.8100) -> will train the basic svc model (written before this) on these hyper parameters.\n\nNext did: After removing stopwords = 'english' parameter in CountVectorizer, validation score improved from 0.80 to 0.81 with C = 0.01, max_iter=946 -->so trained the basic model using these","metadata":{"id":"OJJMzH8Ej3Fu"}},{"cell_type":"markdown","source":"**SVM with SGDClassifier**","metadata":{"id":"kpYPwaGpycWO"}},{"cell_type":"code","source":"#SVM using SGDClasssifier with HPT\n\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.feature_selection import SelectFromModel","metadata":{"id":"CUXjUnHacVxE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgd_svm = SGDClassifier(loss = 'hinge', random_state = 48)","metadata":{"id":"DXmIwGsscOF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'penalty': ['elasticnet'], 'alpha': [0.00001, 0.0001,0.001,0.01], 'max_iter': [1000, 1500, 2000] }\n\ngs_svm= GridSearchCV(sgd_svm, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1, scoring = 'f1_micro')\ngs_svm.fit(merged_X_train_final, merged_y_train_final.toarray()) #removed toarray()","metadata":{"id":"VvV2ttmQcN8Z","outputId":"6896c9b5-0ee3-47a9-dc03-c8f90f56a847"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs_svm.best_score_","metadata":{"id":"5A2ZrDpVcNzZ","outputId":"c8cd6c02-e669-4d34-957c-2cfae9fa0ed9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs_svm.best_params_","metadata":{"id":"d4SVd3aKcNrU","outputId":"de1a020c-efe7-4496-e0a5-431af84cdcb8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training another SGD svm with best parameters on whole data\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import SGDClassifier\n\nsgd2_svm = SGDClassifier(loss = 'hinge', random_state =48, penalty = 'elasticnet', max_iter = 1000, alpha = 0.0001 )\nsgd2_svm.fit(merged_X_train_final, merged_y_train_final.toarray())","metadata":{"id":"aE0eC6qQcNi-","outputId":"d805fcc7-4867-4cac-810b-455ee012edfe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predsgd_svm = sgd2_svm.predict(merged_X_val_final)","metadata":{"id":"W07t86WncNbb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predsvm_test_best = sgd2_svm.predict(merged_test_final)\ny_predsvm_test_best = lb.inverse_transform(y_predsvm_test_best)","metadata":{"id":"LGeNXCugyDPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, f1_score\ncp = classification_report(merged_y_train_final.toarray(), sgd2_svm.predict(merged_X_train_final))\ncm = confusion_matrix(merged_y_train_final.toarray(), sgd2_svm.predict(merged_X_train_final))\nsvmsgdtr = f1_score(merged_y_train_final.toarray(), sgd2_svm.predict(merged_X_train_final), average ='weighted')\nprint('Classification Report for train data using SGD svm with best hyper parameters')\n\nprint(cp)\nprint('The confusion matrix')\nprint(cm)","metadata":{"id":"PRetC8DHcNQo","outputId":"30ea7240-6903-4f4c-82da-7ba5dbe54876"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score\ncp = classification_report(merged_y_val_final.toarray(), y_predsgd_svm)\ncm = confusion_matrix(merged_y_val_final.toarray(), y_predsgd_svm)\nsvmsgd = f1_score(merged_y_val_final.toarray(), y_predsgd_svm, average = 'weighted')\nprint('classification report for validation data using SGD svm with best hyper parameters')\nprint(cp)\n\nprint('The confusion matrix:')\nprint(cm)","metadata":{"id":"JLhPTbf4cNK3","outputId":"022b1f1a-8bf9-4b35-eb23-051e02f1c6f6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not much overfitting for SVM as train ->   0.84\n                           validation ->    0.80   \nthis is with 66,260 features\n\nAfter removing stop_words parameter in CountVectorizer, there are 66,588 features and\n                train --> 0.85\n                validation --> 0.81\n                So both train and validation scores for SVM (with best values for hyper parameters) have increased on removal of stop_words parameter. And also not much overfitting.","metadata":{"id":"MCpzatNelSU-"}},{"cell_type":"code","source":"#Feature selection and PIPELINE using the previously trained sgd2_svm\n\nfrom sklearn.feature_selection import SelectFromModel\n\nsfm = SelectFromModel(sgd2_svm, prefit = True)\nmerged_X_train_fs = sfm.fit_transform(merged_X_train_final)\nmerged_test_fs = sfm.transform(merged_test_final)\n\npipe = Pipeline([('sfm', sfm),\n                     ('sgd2_svm', sgd2_svm)\n                     ])\n\npipe.fit(merged_X_train_final, merged_y_train_final.toarray())","metadata":{"id":"OSxZmxZRcNBM","outputId":"b15380a5-bddd-4205-b0a9-48dd2924178a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(merged_X_train_fs.shape)\nprint(merged_test_fs.shape)","metadata":{"id":"of7kna6E1aCp","outputId":"9159599f-f06a-411e-acde-a9ad2af8917f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(pipe[0].get_feature_names_out())","metadata":{"id":"3s9AwxUhcM7P","outputId":"420d0538-d49c-4843-ea13-8fbbff7c5d1e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_fs_val = pipe.predict(merged_X_val_final)","metadata":{"id":"fh0GDCwCcMzm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ncp = classification_report(merged_y_train_final.toarray(), pipe.predict(merged_X_train_final))\ncm = confusion_matrix(merged_y_train_final.toarray(), pipe.predict(merged_X_train_final))\nsvmsgdfstr = f1_score(merged_y_train_final.toarray(), pipe.predict(merged_X_train_final), average = 'weighted')\nprint('classification report for train data after feature selection (using SGD svm with best hyper parameters)')\nprint(cp)\n\nprint('The confusion matrix')\nprint(cm)","metadata":{"id":"DDDEV8uLcMry","outputId":"74fe5070-7d71-489f-c745-45955c6eac2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score\ncp = classification_report(merged_y_val_final.toarray(), y_pred_fs_val)\ncm = confusion_matrix(merged_y_val_final.toarray(), y_pred_fs_val)\nsvmsgdfs = f1_score(merged_y_val_final.toarray(), y_pred_fs_val, average = 'weighted')\nprint('classification report for validation data after feature selection (using SGD svm with best hyper parameters)')\nprint(cp)\n\nprint('The confusion matrix:')\nprint(cm)","metadata":{"id":"HFdtnKHtcMkM","outputId":"0d401b98-d55f-4c98-cbb8-3d358e582ef7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After feature selection: with 9702 features getting score train->0.84 and validation-> 0.8 (before feature selection also same) -->this is using the best parameters. So no change in the score after feature selection. This is good thing because unnecessary features get eliminated and we can save our computational resources while maintaining the score.\n\n\nAfter removing stop_words parameter in CountVectorizer, and doing feature selection with best parameters, it is giving 9319 features. train-->0.85 and validation -->0.808. So slight decrease in validation score after feature selection.","metadata":{"id":"yeyOwZGEmKxw"}},{"cell_type":"code","source":"#graph to compare the SVM with different steps:\n\nscores = [round(linsvctr, 4), round(linsvc,4), round(svmsgdtr,4), round(svmsgd,4), round(svmsgdfstr,4), round(svmsgdfs, 4)]\nplt.title('f1_score Comparison among SVC (all after HPT)')\nplt.bar(['train LinSVC', 'val LinSVC','train SVM','val SVM','train SVM FS' ,'val SVM FS'], scores)\nplt.xlabel('models')\nplt.ylabel('f1_score')\n\nfor index, value in enumerate(scores):\n    plt.text(index, value, str(value), ha='center', va='bottom')\n\n\nplt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\nplt.show()\n\n","metadata":{"id":"45leNBQWcauV","outputId":"4591d0f9-d639-4262-a0b9-34775dc46daf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"can see that the validation score decreased slightly from 0.813 to 0.808 after feature selection.","metadata":{"id":"SsPnfWt9KYin"}},{"cell_type":"code","source":"y_predsvm_test = pipe.predict(merged_test_final)\ny_predsvm_test = lb.inverse_transform(y_predsvm_test)\n\n# y_predsvm_test_best = lb.inverse_transform(sgd2_svm.predict(merged_test_final))","metadata":{"id":"Tdr_BdK3cMbL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression Model**","metadata":{"id":"c4i4LcPFUlPR"}},{"cell_type":"code","source":"# logistic regression model\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\nlogreg = LogisticRegression(C = 0.1, random_state = 49, max_iter = 80, class_weight = 'balanced')\nlogreg.fit(merged_X_train_final, merged_y_train_final.toarray())","metadata":{"id":"yuii-f7MT60W","outputId":"b0799a7d-1056-4c04-a907-a5be033a57e5","execution":{"iopub.status.busy":"2023-07-30T10:05:57.342866Z","iopub.execute_input":"2023-07-30T10:05:57.344002Z","iopub.status.idle":"2023-07-30T10:10:13.743656Z","shell.execute_reply.started":"2023-07-30T10:05:57.343958Z","shell.execute_reply":"2023-07-30T10:10:13.741275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_lr = logreg.predict(merged_X_val_final)","metadata":{"id":"7N1CeFzqUcUB","execution":{"iopub.status.busy":"2023-07-30T10:10:13.746612Z","iopub.execute_input":"2023-07-30T10:10:13.750372Z","iopub.status.idle":"2023-07-30T10:10:13.770451Z","shell.execute_reply.started":"2023-07-30T10:10:13.750288Z","shell.execute_reply":"2023-07-30T10:10:13.768577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ncp = classification_report(merged_y_train_final.toarray(), logreg.predict(merged_X_train_final))\ncm = confusion_matrix(merged_y_train_final.toarray(), logreg.predict(merged_X_train_final))\nlogisticregtr = f1_score(merged_y_train_final.toarray(), logreg.predict(merged_X_train_final), average = 'weighted')\nprint(cp)\n\nprint('The confusion matrix:')\nprint(cm)","metadata":{"id":"NlTsvaz6jEBb","outputId":"b31422fd-5e6c-424f-dafa-072af374a1fe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score\ncp = classification_report(merged_y_val_final.toarray(), y_pred_lr)\ncm = confusion_matrix(merged_y_val_final.toarray(), y_pred_lr)\nlogisticreg = f1_score(merged_y_val_final.toarray(), y_pred_lr, average = 'weighted')\nprint(cp)\n\nprint('The confusion matrix')\nprint(cm)","metadata":{"id":"ewJMd9PaUfHn","outputId":"741c616a-f67e-443b-8491-13deec9f498e","execution":{"iopub.status.busy":"2023-07-30T10:10:13.773922Z","iopub.execute_input":"2023-07-30T10:10:13.774798Z","iopub.status.idle":"2023-07-30T10:10:18.124773Z","shell.execute_reply.started":"2023-07-30T10:10:13.774736Z","shell.execute_reply":"2023-07-30T10:10:18.123282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For basic LogisticRegression model (without HPT), the f1-score is for train: 0.88 and validation:0.81  --> so model is overfitting.\n\nNext tried to icrease regularisation (C from 1 to 0.01) and decrease max_iter from 100 to 80 --> reduced overfitting (train:0.78, test:0.76), but overall test score decreased.\n\nNext tried with C 0.1 and max_iter 80 --> f1-score for train: 0.84 and validation: 0.80 -> so not much overfitting.\n\nAfter removing stop_words parameter in CountVectorizer, and previous C and max_iter, got f1-score for train: 0.85 and validation: 0.80. So the validation score did not imporve for logistic regression.","metadata":{"id":"CGmzz3Lq19EI"}},{"cell_type":"code","source":"print(y_pred_lr[0])","metadata":{"execution":{"iopub.status.busy":"2023-07-30T10:11:31.515347Z","iopub.execute_input":"2023-07-30T10:11:31.515897Z","iopub.status.idle":"2023-07-30T10:11:31.521999Z","shell.execute_reply.started":"2023-07-30T10:11:31.515862Z","shell.execute_reply":"2023-07-30T10:11:31.520831Z"},"id":"ij9lk-j6gX2f","outputId":"d84ee111-727f-4b97-a33f-086a6ae638df","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LogisticRegression with HPT\n\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.feature_selection import SelectFromModel","metadata":{"id":"JzxHYAx4iYhj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier(loss = 'log_loss', random_state = 49)","metadata":{"id":"CaYROxvu3eUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'penalty': ['elasticnet'], 'alpha': [0.00001, 0.0001,0.001,0.01], 'max_iter': [1000, 1500, 2000] }\n\ngs_logreg= GridSearchCV(sgd, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1, scoring = 'f1_micro')\ngs_logreg.fit(merged_X_train_final, merged_y_train_final.toarray()) #removed toarray()","metadata":{"id":"mRsNYnP2-dNp","outputId":"5fd19652-8dd9-4dbd-bfc0-637384bcb895","execution":{"iopub.status.busy":"2023-07-30T10:10:18.126588Z","iopub.execute_input":"2023-07-30T10:10:18.127262Z","iopub.status.idle":"2023-07-30T10:10:18.133804Z","shell.execute_reply.started":"2023-07-30T10:10:18.127228Z","shell.execute_reply":"2023-07-30T10:10:18.131798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs_logreg.best_score_","metadata":{"id":"YIr9IObdBfOD","execution":{"iopub.status.busy":"2023-07-30T10:10:18.135558Z","iopub.execute_input":"2023-07-30T10:10:18.136126Z","iopub.status.idle":"2023-07-30T10:10:18.150178Z","shell.execute_reply.started":"2023-07-30T10:10:18.136068Z","shell.execute_reply":"2023-07-30T10:10:18.148679Z"},"outputId":"de73e4ff-0d59-4a2c-b486-8507d9b5ec30","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs_logreg.best_params_","metadata":{"id":"9h0b_DpIBuwx","execution":{"iopub.status.busy":"2023-07-30T10:10:18.152669Z","iopub.execute_input":"2023-07-30T10:10:18.153194Z","iopub.status.idle":"2023-07-30T10:10:18.165857Z","shell.execute_reply.started":"2023-07-30T10:10:18.153147Z","shell.execute_reply":"2023-07-30T10:10:18.164165Z"},"outputId":"d78741b2-1132-49c2-ff6d-e425606c7dd7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training another SGD logereg with best parameters on whole data\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import SGDClassifier\n\nsgd2 = SGDClassifier(loss = 'log_loss', random_state = 49, penalty = 'elasticnet', max_iter = 1000 , alpha = 0.0001 )\nsgd2.fit(merged_X_train_final, merged_y_train_final.toarray())","metadata":{"id":"wEIbYBqLkzUs","outputId":"5458ba71-2eb8-47cc-e161-36f1013fc9c6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predsgd_lr = sgd2.predict(merged_X_val_final)","metadata":{"id":"b4LsZ4bMlUG2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score\ncp = classification_report(merged_y_train_final.toarray(), sgd2.predict(merged_X_train_final))\ncm = confusion_matrix(merged_y_train_final.toarray(), sgd2.predict(merged_X_train_final))\nlrsgdtr= f1_score(merged_y_train_final.toarray(), sgd2.predict(merged_X_train_final), average = 'weighted')\n\nprint('Classification Report for train data using SGD logreg with best hyper parameters')\nprint(cp)\nprint('The confusion matrix:')\nprint(cm)","metadata":{"id":"bHT8DlIhpftL","outputId":"d72c64ef-6343-4978-be49-c5eaeed94ca4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score\ncp = classification_report(merged_y_val_final.toarray(), y_predsgd_lr)\ncm = confusion_matrix(merged_y_val_final.toarray(), y_predsgd_lr)\nlrsgd = f1_score(merged_y_val_final.toarray(), y_predsgd_lr, average = 'weighted')\n\nprint('classification report for validation data using SGD logreg with best hyper parameters')\nprint(cp)\n\nprint('The confusion matix:')\nprint(cm)","metadata":{"id":"w6uT1pTclW15","outputId":"2293cc96-cbcc-4a78-d129-580f4978bff6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not much overfitting for logreg as train ->0.83\n                                  validation ->0.80\n\nThis is for 66,260 features\n\nAfter removing stop_words parameter from CountVectorizer, got 66,588 features.\nFor this train --> 0.83 and validation -->0.80. So no change in the train and validation scores for logistic regression (with best hyper parameters) on removal of stop_words parameter.\n\n        ","metadata":{"id":"sve8wk3hqKvk"}},{"cell_type":"code","source":"#Feature selection and PIPELINE using the previously trained sgd2\n\nfrom sklearn.feature_selection import SelectFromModel\n\nsfm = SelectFromModel(sgd2, prefit = True)\nmerged_X_train_fs = sfm.fit_transform(merged_X_train_final)\nmerged_test_fs = sfm.transform(merged_test_final)\n\npipe2 = Pipeline([('sfm', sfm),\n                     ('sgd2', sgd2)\n                     ])\n\npipe2.fit(merged_X_train_final, merged_y_train_final.toarray())","metadata":{"id":"qnmk4iS5su_1","outputId":"16ba3e37-2a42-40f9-8e55-323fa53e97bf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(merged_X_train_fs.shape)\nprint(merged_test_fs.shape)","metadata":{"id":"PkMMsnnn5aQL","outputId":"1c9293d2-fd59-4e75-b467-89c1418467f2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(pipe2[0].get_feature_names_out())","metadata":{"id":"2Mz2H6v1xEVw","outputId":"0c8f743d-986c-4b85-dc77-a1ea0d19619c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predlr_fs_val = pipe2.predict(merged_X_val_final)","metadata":{"id":"HrBHPQTxvo-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score\ncp = classification_report(merged_y_train_final.toarray(), pipe2.predict(merged_X_train_final))\ncm = confusion_matrix(merged_y_train_final.toarray(), pipe2.predict(merged_X_train_final))\nlrsgdfstr = f1_score(merged_y_train_final.toarray(), pipe2.predict(merged_X_train_final), average = 'weighted')\n\nprint('classification report for train data after feature selection (using SGD logreg with best hyper parameters)')\nprint(cp)\n\nprint('The confusion matrix')\nprint(cm)","metadata":{"id":"NequQvh1wE8I","outputId":"6e9e84ce-abe5-4094-d426-f61f4b73a498"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score\ncp = classification_report(merged_y_val_final.toarray(), y_predlr_fs_val)\ncm = confusion_matrix(merged_y_val_final.toarray(), y_predlr_fs_val)\nlrsgdfs = f1_score(merged_y_val_final.toarray(), y_predlr_fs_val, average = 'weighted')\n\nprint('classification report for validation data after feature selection (using SGD logreg with best hyper parameters)')\nprint(cp)\n\nprint('The confusion matrix')\nprint(cm)","metadata":{"id":"CPS23up5wGdT","outputId":"0f094302-5725-4a75-c607-67478704fb4b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After feature selection: with 6628 features also getting score train-> 0.83 and validation-> 0.80  (before feature selection also it is 0.80)   -->this is using the best parameters. So no change in the score after feature selection. This is good thing because unnecessary features get eliminated and we can save our computational resources while maintaining the score.\n\nAfter removing the stop_words parameter in CountVectorizer, and doing feature selection, giving 6630 features. With f1-score for train --> 0.83 and validation --> 0.80. This is same as before applying feature selection.","metadata":{"id":"Nwm6Aehjynhr"}},{"cell_type":"code","source":"#Comparison of score among LogisticRegression\n\nscores = [round(logisticregtr,4), round(logisticreg,4), round(lrsgdtr,4), round(lrsgd,4), round(lrsgdfstr,4), round(lrsgdfs,4)]\nplt.title('f1_score comparison among Logistic regression')\nplt.bar(['train-basic', 'val-basic','train-HPT', 'val-HPT', 'train-HPT & FS', 'val-HPT & FS'], scores)\nplt.xlabel('models')\nplt.ylabel('f1_score')\n\nfor index, value in enumerate(scores):\n    plt.text(index, value, str(value), ha='center', va='bottom')\n\n\nplt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\nplt.show()\n\n","metadata":{"id":"XuBmYNYFGS3L","outputId":"77f9a969-3c3e-4b84-e764-8d3f57e214a5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"can see the score remains almost same even after feature selection.","metadata":{"id":"-4KB1Qp0K2EG"}},{"cell_type":"code","source":"y_predlr_test = pipe2.predict(merged_test_final)\ny_predlr_test = lb.inverse_transform(y_predlr_test)","metadata":{"id":"Cq8cewoeUhXU","execution":{"iopub.status.busy":"2023-07-30T10:10:18.168131Z","iopub.execute_input":"2023-07-30T10:10:18.168739Z","iopub.status.idle":"2023-07-30T10:10:18.185801Z","shell.execute_reply.started":"2023-07-30T10:10:18.168698Z","shell.execute_reply":"2023-07-30T10:10:18.184001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Naive Bayes model","metadata":{"id":"CVi-zYrLXBNu"}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nmnb = MultinomialNB()\nmnb.fit(merged_X_train_final, merged_y_train_final.toarray())","metadata":{"id":"hfZ_-IU7WvDY","execution":{"iopub.status.busy":"2023-07-30T10:10:18.187961Z","iopub.execute_input":"2023-07-30T10:10:18.188486Z","iopub.status.idle":"2023-07-30T10:10:18.194167Z","shell.execute_reply.started":"2023-07-30T10:10:18.188404Z","shell.execute_reply":"2023-07-30T10:10:18.193085Z"},"outputId":"2e833e63-6d75-4f73-8df3-6f7137d2cc0e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_mnb = mnb.predict(merged_X_val_final)","metadata":{"id":"2ONfSof4XIBM","execution":{"iopub.status.busy":"2023-07-30T10:10:18.196220Z","iopub.execute_input":"2023-07-30T10:10:18.196965Z","iopub.status.idle":"2023-07-30T10:10:18.210240Z","shell.execute_reply.started":"2023-07-30T10:10:18.196889Z","shell.execute_reply":"2023-07-30T10:10:18.208786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score\ncp = classification_report(merged_y_train_final.toarray(), mnb.predict(merged_X_train_final))\ncm = confusion_matrix(merged_y_train_final.toarray(), mnb.predict(merged_X_train_final))\nmultinbtr = f1_score(merged_y_train_final.toarray(), mnb.predict(merged_X_train_final), average = 'weighted')\nprint(cp)\n\nprint('The confusion matrix:')\nprint(cm)","metadata":{"id":"9tRKWOTqBhdy","outputId":"935d1cbe-f54d-48e1-cebd-56a19e63d674"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score\ncp = classification_report(merged_y_val_final.toarray(), y_pred_mnb)\ncm = confusion_matrix(merged_y_val_final.toarray(), y_pred_mnb)\nmultinb = f1_score(merged_y_val_final.toarray(), y_pred_mnb, average = 'weighted')\n\nprint(cp)\n\nprint('The confusion matrix:')\nprint(cm)","metadata":{"id":"DKLvDkcGXIm0","execution":{"iopub.status.busy":"2023-07-30T10:10:18.211568Z","iopub.execute_input":"2023-07-30T10:10:18.211967Z","iopub.status.idle":"2023-07-30T10:10:18.223921Z","shell.execute_reply.started":"2023-07-30T10:10:18.211934Z","shell.execute_reply":"2023-07-30T10:10:18.222697Z"},"outputId":"d4e8b906-7330-4d8a-b513-54b599191ba3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Basic MNB f1-score: train:0.83, validation: 0.78 --> so overfitting\nNext tried HPT\n\nAfter removing stop_words parameter in CountVectorizer, giving the same train and validation f1-score as before.","metadata":{"id":"00G3CLMJQ0Nb"}},{"cell_type":"code","source":"# # Multinomial Naive Bayes with HPT\nfrom sklearn.naive_bayes import MultinomialNB\n\nmnb2 = MultinomialNB()\n\nfrom sklearn.model_selection import RandomizedSearchCV\nparam_dist = {'alpha': [0.001, 0.1, 1], 'force_alpha': [True, False]}\n\nrs_mnb = RandomizedSearchCV(mnb2, param_distributions = param_dist, scoring = 'f1_micro', cv = 5, n_jobs =-1)\nrs_mnb.fit(merged_X_train_final, merged_y_train_final.toarray())","metadata":{"id":"7Hj--zBwRLz2","outputId":"c0bdff63-af96-4e3c-c100-34854d2196b9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"best score:\", rs_mnb.best_score_)\nprint(\"best parameters:\", rs_mnb.best_params_)","metadata":{"id":"DvjVs4pzRMgN","outputId":"94d9a056-a1be-417c-d642-4f450cbee9f2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training another MNB with best parameters on whole data\n\nfrom sklearn.metrics import classification_report\nmnb3 = MultinomialNB(alpha =1, force_alpha = True)\nmnb3.fit(merged_X_train_final, merged_y_train_final.toarray())","metadata":{"id":"AU6aILGUR0a8","outputId":"3f90cb1a-ac0d-473f-b64e-37a4d0ff435e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predmnb_val = mnb3.predict(merged_X_val_final)","metadata":{"id":"dxcOfgEBUVc2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ncp = classification_report(merged_y_train_final.toarray(), mnb3.predict(merged_X_train_final))\ncm = confusion_matrix(merged_y_train_final.toarray(), mnb3.predict(merged_X_train_final))\nmultinbhpttr = f1_score(merged_y_train_final.toarray(), mnb3.predict(merged_X_train_final), average = 'weighted')\n\nprint('Classification Report for train data using MultinomialNB with best hyper parameters')\nprint(cp)\n\nprint('The confusion matrix:')\nprint(cm)","metadata":{"id":"8aeGE5vNUaMh","outputId":"94d0192d-baf7-4f4d-b8c6-28b284f0e49e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score\ncp = classification_report(merged_y_val_final.toarray(), y_predmnb_val)\ncm = confusion_matrix(merged_y_val_final.toarray(), y_predmnb_val)\nmultinbhpt = f1_score(merged_y_val_final.toarray(), y_predmnb_val, average = 'weighted')\n\nprint('classification report for validation data using MultinomialNB with best hyper parameters')\nprint(cp)\n\nprint('The confusion matrix:')\nprint(cm)","metadata":{"id":"Br0Gpz1EUZ8Y","outputId":"eaa087e3-833f-4eba-969c-983aecaa27dd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Did not perform feature selection because the validation score was not improving much even afte Hyper parameter tuning.","metadata":{"id":"ibpFWRr4MJ8n"}},{"cell_type":"code","source":"#Comparison of score among MultinomialNB\n\nscores = [round(multinbtr,4), round(multinb,4), round(multinbhpttr,4), round(multinbhpt,4)]\nplt.title('f1_score comparison among MNB')\nplt.bar(['train-basic', 'val-basic','train-HPT', 'val-HPT'], scores)\nplt.xlabel('models')\nplt.ylabel('f1_score')\n\nfor index, value in enumerate(scores):\n    plt.text(index, value, str(value), ha='center', va='bottom')\n\n\nplt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\nplt.show()\n","metadata":{"id":"OhJzb5dSLxYx","outputId":"129659d2-cb49-492b-dc4b-9076daa59788"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predmnb_test = mnb3.predict(merged_test_final)\ny_predmnb_test = lb.inverse_transform(y_predmnb_test)","metadata":{"id":"qs3-OULaXLYL","execution":{"iopub.status.busy":"2023-07-30T10:10:18.225365Z","iopub.execute_input":"2023-07-30T10:10:18.225773Z","iopub.status.idle":"2023-07-30T10:10:18.237758Z","shell.execute_reply.started":"2023-07-30T10:10:18.225740Z","shell.execute_reply":"2023-07-30T10:10:18.236536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Decision Tree","metadata":{"id":"u5g-mPySbpES"}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(random_state=0)\ndtc.fit(merged_X_train_final, merged_y_train_final.toarray())","metadata":{"id":"dbgcaDBWNsAA","outputId":"beb10b75-ba4b-48d1-e569-2712385a6880"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_dtc = dtc.predict(merged_X_val_final)","metadata":{"id":"_w7YL4jDN2Kz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, f1_score\ncp = classification_report(merged_y_train_final.toarray(), dtc.predict(merged_X_train_final))\ncm = confusion_matrix(merged_y_train_final.toarray(), dtc.predict(merged_X_train_final))\ndecttr = f1_score(merged_y_train_final.toarray(), dtc.predict(merged_X_train_final), average = 'weighted')\nprint(cp)\nprint('The Confusion Matrix:')\nprint(cm)","metadata":{"id":"bYgFBN9sN5dT","outputId":"862973db-e5b2-4044-9a89-3b82413c07bb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, f1_score\ncp = classification_report(merged_y_val_final.toarray(), y_pred_dtc)\ncm = confusion_matrix(merged_y_val_final.toarray(), y_pred_dtc)\ndect = f1_score(merged_y_val_final.toarray(), y_pred_dtc, average = 'weighted')\n\nprint(cp)\nprint('The confusion matrix:')\nprint(cm)","metadata":{"id":"RhgsuAmaN8p9","outputId":"a0d6c553-a532-4ef7-ac59-fc6e7ac2ea38"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On basic DecisionTreeClassifier, f1-score for train -->1, validation-->0.7, so overfitting.","metadata":{"id":"k9XYjcpwSFsJ"}},{"cell_type":"code","source":"# # DecisionTreeClassifier with HPT\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\ndtc2 = DecisionTreeClassifier(random_state=0)\n\nparam_dist = {'max_depth': [2,3,5], 'min_samples_leaf': [0.5, 1,2], 'min_samples_split': [2,5,7]}\n\nrs_dtc = RandomizedSearchCV(dtc2, param_distributions = param_dist, scoring = 'f1_micro', cv = 5, n_jobs =-1)\nrs_dtc.fit(merged_X_train_final, merged_y_train_final.toarray())","metadata":{"id":"VFUFN8FLN_oO","outputId":"81bd436c-76f7-4948-d0d5-f49810ed70c3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"best score:\", rs_dtc.best_score_)\nprint(\"best parameters:\", rs_dtc.best_params_)","metadata":{"id":"GLFZwTaEOIA4","outputId":"b3f57e9e-4469-4098-a320-9b3de9b00f70"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training another DTC with best parameters on the whole data\n\nfrom sklearn.metrics import classification_report\ndtc3 = DecisionTreeClassifier(random_state =0 ,min_samples_split = 7, min_samples_leaf= 1, max_depth =5 )\ndtc3.fit(merged_X_train_final, merged_y_train_final.toarray())","metadata":{"id":"1uOWcSfNOIjW","outputId":"6217bb9b-81be-4c9d-ea16-88df7a1ebaab"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preddtc_val = dtc3.predict(merged_X_val_final)","metadata":{"id":"bgtbKIiAOLHx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, f1_score\ncp = classification_report(merged_y_train_final.toarray(), dtc3.predict(merged_X_train_final))\ncm = confusion_matrix(merged_y_train_final.toarray(), dtc3.predict(merged_X_train_final))\ndecthpttr = f1_score(merged_y_train_final.toarray(), dtc3.predict(merged_X_train_final), average = 'weighted')\n\nprint('Classification Report for train data using MultinomialNB with best hyper parameters')\nprint(cp)\nprint('The confusion matrix:')\nprint(cm)","metadata":{"id":"6I83akLUONmi","outputId":"d69af9ce-3681-40ec-92d5-6b3297c07a3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ncp = classification_report(merged_y_val_final.toarray(), y_preddtc_val)\ncm = confusion_matrix(merged_y_val_final.toarray(), y_preddtc_val)\ndecthpt = f1_score(merged_y_val_final.toarray(), y_preddtc_val, average = 'weighted')\n\nprint('classification report for validation data using MultinomialNB with best hyper parameters')\nprint(cp)\nprint('The confusion matrix:')\nprint(cm)","metadata":{"id":"PfzQX3hiOVXM","outputId":"10feb347-6566-478c-f7cd-ee0cffbf66e5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After Hyper parameter tuning, the f1-score of train\n:0.70 and validation: 0.69 --> so reduced the overfitting. The validation score reduced from 0.70 to 0.69.","metadata":{"id":"Li7ORn8_tLtI"}},{"cell_type":"code","source":"#Comparison of score among DecisionTree\n\nscores = [round(decttr,4), round(dect,4), round(decthpttr,4), round(decthpt,4)]\nplt.title('f1_score comparison among DecisionTreeClassifier')\nplt.bar(['train-basic', 'val-basic','train-HPT', 'val-HPT'], scores)\nplt.xlabel('models')\nplt.ylabel('f1_score')\n\nfor index, value in enumerate(scores):\n    plt.text(index, value, str(value), ha='center', va='bottom')\n\n\nplt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\nplt.show()","metadata":{"id":"nULmQYHclkZM","outputId":"68cb2367-47dc-46d5-e6f8-023ceb557924"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preddtc_test = dtc3.predict(merged_test_final)\ny_preddtc_test = lb.inverse_transform(y_preddtc_test)","metadata":{"id":"TRU2yc8WOZlF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Comparison of best validation score among all\n\nscores = [round(svmsgd,4), round(lrsgdfs,4), round(multinbhpt,4), round(decthpt,4)]\nplt.title('Best validation f1_score comparison among all')\nplt.bar(['SVM HPT(no FS)', 'LR HPT & FS','MNB HPT', 'DT HPT'], scores)\nplt.xlabel('models')\nplt.ylabel('f1_score')\n\nfor index, value in enumerate(scores):\n    plt.text(index, value, str(value), ha='center', va='bottom')\n\n\nplt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\nplt.show()","metadata":{"id":"VW0IJWwppQ-O","outputId":"949e7b21-2376-469e-a3c4-b6b395287bef"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Can see that among all the SVM model with hyper parameter tuning and without feature selection is giving the best validation score at 0.8133, followed by LogisticRegression model with hyper parameter tuning and with feature selection at 0.8047. The Multinomial Naive Bayes with hyper parameter tuning and Decision Tree with hyper parameter tuning are giving best score of 0.7834 and 0.6941 respectively.","metadata":{"id":"wDMX6teosL9T"}},{"cell_type":"code","source":"submission = pd.DataFrame(columns = ['id', 'sentiment'])\nsubmission[\"id\"] = [i for i in range(len(y_predsvm_test_best))]\nsubmission[\"sentiment\"] = y_predsvm_test_best\nsubmission.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T10:10:18.307271Z","iopub.execute_input":"2023-07-30T10:10:18.307734Z","iopub.status.idle":"2023-07-30T10:10:18.548341Z","shell.execute_reply.started":"2023-07-30T10:10:18.307697Z","shell.execute_reply":"2023-07-30T10:10:18.546949Z"},"id":"XDbo9OcvgX2i","trusted":true},"execution_count":null,"outputs":[]}]}